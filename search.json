[
  {
    "objectID": "src/Time_Series_Analysis.html#data-preparation",
    "href": "src/Time_Series_Analysis.html#data-preparation",
    "title": "Time Series Analysis - Anomaly Detection",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n# Read already prepared dataset\nmfg_df = pd.read_csv('..\\data\\processed\\mfg_time_series.csv')\nmfg_df.head()\n\n\n\n\n\n\n\n\nMachine_ID\nTimestamp\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_%\nQuality_Control_Defect_Rate_%\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_%\n\n\n\n\n0\n1\n2024-01-01 00:00:00\n59.027691\n3.273465\n6.871082\n23.445160\n2.308684\n4.684600\n349.632925\n0.392129\n6.939090\n\n\n1\n1\n2024-01-01 12:00:00\n63.907350\n2.021363\n4.978956\n30.675345\n2.408669\n5.366573\n321.184645\n0.410275\n7.534709\n\n\n2\n1\n2024-01-02 00:00:00\n51.808176\n2.047471\n6.029171\n31.269433\n1.834945\n6.899124\n363.738970\n0.522837\n6.831884\n\n\n3\n1\n2024-01-02 12:00:00\n57.371817\n2.646207\n5.790383\n28.433255\n3.047640\n5.324885\n294.783106\n0.425722\n5.794327\n\n\n4\n1\n2024-01-03 00:00:00\n56.866279\n2.500103\n7.077736\n29.890366\n3.134400\n5.549209\n302.052996\n0.307520\n7.169374\n\n\n\n\n\n\n\n\n# Data Preparation - Setting Correct Columns Types\nmfg_df['Machine_ID'] = mfg_df['Machine_ID'].astype('category')\nmfg_df.columns = mfg_df.columns.str.replace('%', 'perc').astype(str)\nmfg_df['Timestamp'] = pd.to_datetime(mfg_df['Timestamp'])\n\nmfg_df.head()\n\n\n\n\n\n\n\n\n\nMachine_ID\nTimestamp\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_perc\nQuality_Control_Defect_Rate_perc\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_perc\n\n\n\n\n0\n1\n2024-01-01 00:00:00\n59.027691\n3.273465\n6.871082\n23.445160\n2.308684\n4.684600\n349.632925\n0.392129\n6.939090\n\n\n1\n1\n2024-01-01 12:00:00\n63.907350\n2.021363\n4.978956\n30.675345\n2.408669\n5.366573\n321.184645\n0.410275\n7.534709\n\n\n2\n1\n2024-01-02 00:00:00\n51.808176\n2.047471\n6.029171\n31.269433\n1.834945\n6.899124\n363.738970\n0.522837\n6.831884\n\n\n3\n1\n2024-01-02 12:00:00\n57.371817\n2.646207\n5.790383\n28.433255\n3.047640\n5.324885\n294.783106\n0.425722\n5.794327\n\n\n4\n1\n2024-01-03 00:00:00\n56.866279\n2.500103\n7.077736\n29.890366\n3.134400\n5.549209\n302.052996\n0.307520\n7.169374\n\n\n\n\n\n\n\n\nOutliers report\nTemperature_C: 30 outliers  Vibration_Hz: 27 outliers Power_Consumption_kW: 43 outliers Network_Latency_ms: 26 outliers Packet_Loss_%: 28 outliers Quality_Control_Defect_Rate_%: 39 outliers Production_Speed_units_per_hr: 31 outliers Predictive_Maintenance_Score: 33 outliers Error_Rate_%: 29 outliers\n\n# Plot the Power_Consumption_Kw values for Machine 1\n\nplt_df = mfg_df[mfg_df['Machine_ID']==4]\nplt.figure(figsize=(14, 5))\nplt.plot(plt_df[\"Timestamp\"], plt_df[\"Power_Consumption_kW\"], marker=\"o\", markersize=3, linestyle=\"-\")\n\nplt.title(\"Power_Consumption_kW Over Time\", fontsize=14)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Power_Consumption_kW\")\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n\n\n\n\n\n\n\n\n# Filter Machine 1 for Next Steps and define timestamp column as time\nts_df = mfg_df.copy()\nts_df = ts_df[ts_df['Machine_ID']==1]\n\nts_df['Timestamp'] = pd.to_datetime(ts_df['Timestamp'])\nts_df.set_index('Timestamp', inplace = True)\nts_df.head()\n\n\n\n\n\n\n\n\nMachine_ID\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_perc\nQuality_Control_Defect_Rate_perc\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_perc\n\n\nTimestamp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-01-01 00:00:00\n1\n59.027691\n3.273465\n6.871082\n23.445160\n2.308684\n4.684600\n349.632925\n0.392129\n6.939090\n\n\n2024-01-01 12:00:00\n1\n63.907350\n2.021363\n4.978956\n30.675345\n2.408669\n5.366573\n321.184645\n0.410275\n7.534709\n\n\n2024-01-02 00:00:00\n1\n51.808176\n2.047471\n6.029171\n31.269433\n1.834945\n6.899124\n363.738970\n0.522837\n6.831884\n\n\n2024-01-02 12:00:00\n1\n57.371817\n2.646207\n5.790383\n28.433255\n3.047640\n5.324885\n294.783106\n0.425722\n5.794327\n\n\n2024-01-03 00:00:00\n1\n56.866279\n2.500103\n7.077736\n29.890366\n3.134400\n5.549209\n302.052996\n0.307520\n7.169374\n\n\n\n\n\n\n\n\n#https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Decompose the time series\ndecomposition = seasonal_decompose(ts_df['Power_Consumption_kW'], model = 'additive',period=14)\n\n# Plot the decomposed components\ndecomposition.plot()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Improved Chart, getting all data individully to improve visualization from standard charts.\nfig, axes = plt.subplots(4, 1, figsize=(8, 6), sharex=True)\n\naxes[0].plot(decomposition.observed, marker=\"o\", markersize=3, linestyle=\"-\")\naxes[0].set_ylabel('Observed')\n\naxes[1].plot(decomposition.trend, marker=\"o\", markersize=3, linestyle=\"-\")\naxes[1].set_ylabel('Trend')\n\naxes[2].plot(decomposition.seasonal, marker=\"o\", markersize=3, linestyle=\"-\")\naxes[2].set_ylabel('Seasonal')\n\naxes[3].plot(decomposition.resid, marker=\"o\", markersize=3, linestyle=\"\")\naxes[3].axhline(0, color='red', linestyle='--', linewidth=1)  \naxes[3].set_ylabel('Residual')\naxes[3].set_xlabel('Date')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Anomalies in residuals\nresiduals = decomposition.resid.dropna()\nthreshold = 2 * residuals.std()\nanomalies = np.abs(residuals) &gt; threshold # Results\nanomalies.sum()\n\n4\n\n\n\n# Plot the Power_Consumption_Kw values for Machine 1\n# Highlight points identfied by the rule of 2X std dev.\n\nplt_df = ts_df.copy()\nplt_df['Power_Consumption_kW_anomalies'] = anomalies\nplt.figure(figsize=(10, 4))\nplt.plot(plt_df['Power_Consumption_kW'], label='Observed')\nsns.scatterplot(plt_df,x=plt_df.index,y=plt_df['Power_Consumption_kW'],hue=plt_df['Power_Consumption_kW_anomalies'])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Source: https://datamineaz.org/exercises/ex-09.html\n# Test to check if time Series is Stationary (stays constant over time)\nfrom statsmodels.tsa.stattools import adfuller\n\n# Perform Augmented Dickey-Fuller test\nresult = adfuller(ts_df['Power_Consumption_kW'])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\n\n# Interpretation\nif result[1] &gt; 0.05:\n    print(\"Series is not stationary\")\nelse:\n    print(\"Series is stationary\")\n\nADF Statistic: -11.694420\np-value: 0.000000\nSeries is stationary\n\n\nWhat are p, d, and q?\np = order of the AutoRegressive (AR) part (how many past values algo will use)\nd = degree of differencing (how many times you subtract previous values to make series stationary)\nq = order of the Moving Average (MA) part (how many past errors you include)\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Fit ARIMA model\narima_model = ARIMA(ts_df['Power_Consumption_kW'], order=(1,1,1)) # Randomly selected 1,1,1 for P,D,Q - will use auto-ARIMA\nfitted_model = arima_model.fit()\n\n# Get residuals\nresiduals = fitted_model.resid\nthreshold = 2.5 * residuals.std()\nanomalies = np.abs(residuals) &gt; threshold\nanomalies.sum()\n\nc:\\Users\\cesar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency 12h will be used.\n  self._init_dates(dates, freq)\nc:\\Users\\cesar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency 12h will be used.\n  self._init_dates(dates, freq)\nc:\\Users\\cesar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency 12h will be used.\n  self._init_dates(dates, freq)\n\n\n1\n\n\n\n#Fitting auto-arima model\nfrom pmdarima import auto_arima\nauto_model = auto_arima(ts_df['Power_Consumption_kW'], \n                        seasonal=False,\n \n                        error_action = 'ignore',  \n                        suppress_warnings = True, \n                        stepwise = True)\nprint(auto_model.summary())\nprint(f\"Best (p, d, q): ({auto_model.order[0]}, {auto_model.order[1]}, {auto_model.order[2]})\")\n\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                      y   No. Observations:                  139\nModel:                        SARIMAX   Log Likelihood                -149.194\nDate:                Fri, 15 Aug 2025   AIC                            302.388\nTime:                        13:39:47   BIC                            308.257\nSample:                    01-01-2024   HQIC                           304.773\n                         - 03-10-2024                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      5.8176      0.060     96.802      0.000       5.700       5.935\nsigma2         0.5010      0.059      8.537      0.000       0.386       0.616\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                 0.16\nProb(Q):                              0.98   Prob(JB):                         0.92\nHeteroskedasticity (H):               1.37   Skew:                             0.07\nProb(H) (two-sided):                  0.28   Kurtosis:                         3.10\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nBest (p, d, q): (0, 0, 0)\n\n\n\n#Find the Anomalies based on the difference between predicted and observed.\n\nfitted_values = auto_model.predict_in_sample()\n#fitted_values = auto_model.predict()\nresiduals = ts_df['Power_Consumption_kW'] - fitted_values\nthreshold = 2 * residuals.std()\nanomalies = np.abs(residuals) &gt; threshold\nts_df['Power_Consumption_kW_anomalies'] =anomalies\n\n\n# Plot the Power_Consumption_Kw values for Machine 1\n# Highlight points identfied by the rule of 2X std dev.\n\nplt_df = ts_df.copy()\nplt.figure(figsize=(10, 4))\nplt.plot(ts_df['Power_Consumption_kW'], label='Observed')\nplt.plot(fitted_values, label='Fitted')\nsns.scatterplot(ts_df,x=ts_df.index,y=ts_df['Power_Consumption_kW'],hue=ts_df['Power_Consumption_kW_anomalies'])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# This was an attempt to use LSTM, however, not used for final report as understand of the algo is not clear to me.\n# Source: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Scale data\nscaler = MinMaxScaler(feature_range=(0, 1))\nts_scaled = scaler.fit_transform(ts_df['Power_Consumption_kW'].to_numpy().reshape(-1, 1))\n\n# Create sequences\n# Source: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n#The function takes two arguments: the dataset, which is a NumPy array you want to convert into a dataset, \n# and the look_back, which is the number of previous time steps to use as input variables to predict the next time period—in this case, defaulted to 1.\ndef create_sequences(data, seq_length):\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i+seq_length])\n        y.append(data[i+seq_length])\n    return np.array(X), np.array(y)\n\nseq_length = 10\nX, y = create_sequences(ts_scaled, seq_length)\n\n# Reshape X to be [samples, time steps, features]\nX = X.reshape((X.shape[0], X.shape[1], 1))\n\n# Build LSTM model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(seq_length, 1)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train model\nmodel.fit(X, y, epochs=50, batch_size=32, verbose=1)\n\nEpoch 1/50\n\n\n\n# Create similar chart as with ARIMA model. \ny_pred = model.predict(X)\n# Plot the Power_Consumption_Kw values for Machine 1\n\nplt_df = ts_df.copy()\nplt.figure(figsize=(10, 4))\nplt.plot(y, label='Observed')\nplt.plot(y_pred, label='Fitted')\nplt.legend()\nplt.show()\n\n\n5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step"
  },
  {
    "objectID": "src/Data_Exploration_PDM.html",
    "href": "src/Data_Exploration_PDM.html",
    "title": "Data Exploration/Cleaning for Predictive Maintenance",
    "section": "",
    "text": "Author: Cesar Castro M\n\n\nDate: 08/11/2025\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\n\n\nmfg_data = pd.read_csv('..\\data\\predictive_maintenance.csv')\n\nSource: https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification\n - UID: unique identifier ranging from 1 to 10000  - productID: consisting of a letter L, M, or H for low (50% of all products), medium (30%), and high (20%) as product quality variants and a variant-specific serial number - air temperature [K]: generated using a random walk process later normalized to a standard deviation of 2 K around 300 K - process temperature [K]: generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K. - rotational speed [rpm]: calculated from powepower of 2860 W, overlaid with a normally distributed noise - torque [Nm]: torque values are normally distributed around 40 Nm with an Ïƒ = 10 Nm and no negative values. - tool wear [min]: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process. and a ‘machine failure’ label that indicates, whether the machine has failed in this particular data point for any of the following failure modes are true.\n\nmfg_data.head()\n\n\n\n\n\n\n\n\nUDI\nProduct ID\nType\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nTarget\nFailure Type\n\n\n\n\n0\n1\nM14860\nM\n298.1\n308.6\n1551\n42.8\n0\n0\nNo Failure\n\n\n1\n2\nL47181\nL\n298.2\n308.7\n1408\n46.3\n3\n0\nNo Failure\n\n\n2\n3\nL47182\nL\n298.1\n308.5\n1498\n49.4\n5\n0\nNo Failure\n\n\n3\n4\nL47183\nL\n298.2\n308.6\n1433\n39.5\n7\n0\nNo Failure\n\n\n4\n5\nL47184\nL\n298.2\n308.7\n1408\n40.0\n9\n0\nNo Failure\n\n\n\n\n\n\n\n\n# Setting right feature type\nmfg_data['UDI'] = mfg_data['UDI'].astype('object')\n\n\n#Find Categorical Features on childcare_costs dataset\ndef cat_features_explore(df):\n    def calc_unique_counts(df,col_name):\n        col = df[col_name]\n        unique_val = col.unique()\n        plt.figure(figsize=(12,4))\n        sns.countplot(data=df, x=col_name)\n        plt.title(f'Categories for {col_name}')\n        plt.tight_layout()\n        plt.show()\n        return len(unique_val)\n\n    cat_cols = mfg_data.select_dtypes(include=\"object\").columns.to_list()\n    count_dic = {col:calc_unique_counts(mfg_data,col) for col in cat_cols}\n    print(count_dic)    \n \ncat_features_explore(mfg_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'UDI': 10000, 'Product ID': 10000, 'Type': 3, 'Failure Type': 6}\n\n\n\ndef plot_hist_qq(df,col_name):\n    col = df[col_name]\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2,nrows=1,figsize=(8, 4))\n    sns.histplot(col, linewidth=1,ax=ax1)\n    #sns.kdeplot(col, linewidth=5,ax=ax1)\n    ax1.set_title(col_name + ' Histogram')\n\n    sm.qqplot(col,line='s',ax=ax2)\n    ax2.set_title(col_name + ' Q-Q Plot')\n    plt.tight_layout()\n    plt.show()\n\ndef calculate_stats(df,col_name):\n    print (f\"{col_name} Mean: {df[col_name].mean():.2f}\")\n    print (f\"{col_name} Median: {df[col_name].median():.2f}\")\n    print (f\"{col_name} Max: {df[col_name].max():.2f}\")\n    print (f\"{col_name} Min: {df[col_name].min():.2f}\")\n    print (f\"{col_name} Standard Deviation: {df[col_name].std():.2f}\")    \n    print (f\"{col_name} Number of Points: {len(df[col_name]):.0f}\")    \n\nfor col in mfg_data.select_dtypes(include=\"number\").columns.to_list():\n    plot_hist_qq(mfg_data,col) \n    calculate_stats(mfg_data,col)\n\n\n\n\n\n\n\n\nAir temperature [K] Mean: 300.00\nAir temperature [K] Median: 300.10\nAir temperature [K] Max: 304.50\nAir temperature [K] Min: 295.30\nAir temperature [K] Standard Deviation: 2.00\nAir temperature [K] Number of Points: 10000\n\n\n\n\n\n\n\n\n\nProcess temperature [K] Mean: 310.01\nProcess temperature [K] Median: 310.10\nProcess temperature [K] Max: 313.80\nProcess temperature [K] Min: 305.70\nProcess temperature [K] Standard Deviation: 1.48\nProcess temperature [K] Number of Points: 10000\n\n\n\n\n\n\n\n\n\nRotational speed [rpm] Mean: 1538.78\nRotational speed [rpm] Median: 1503.00\nRotational speed [rpm] Max: 2886.00\nRotational speed [rpm] Min: 1168.00\nRotational speed [rpm] Standard Deviation: 179.28\nRotational speed [rpm] Number of Points: 10000\n\n\n\n\n\n\n\n\n\nTorque [Nm] Mean: 39.99\nTorque [Nm] Median: 40.10\nTorque [Nm] Max: 76.60\nTorque [Nm] Min: 3.80\nTorque [Nm] Standard Deviation: 9.97\nTorque [Nm] Number of Points: 10000\n\n\n\n\n\n\n\n\n\nTool wear [min] Mean: 107.95\nTool wear [min] Median: 108.00\nTool wear [min] Max: 253.00\nTool wear [min] Min: 0.00\nTool wear [min] Standard Deviation: 63.65\nTool wear [min] Number of Points: 10000\n\n\n\n\n\n\n\n\n\nTarget Mean: 0.03\nTarget Median: 0.00\nTarget Max: 1.00\nTarget Min: 0.00\nTarget Standard Deviation: 0.18\nTarget Number of Points: 10000\n\n\n\nnum_cols = mfg_data.select_dtypes(include=\"number\").columns.to_list()\nplot_df = mfg_data[num_cols]\nplt.figure(figsize=(10,6))\nsns.pairplot(plot_df, diag_kind='kde',hue='Target')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n#Standardization\nfrom sklearn.preprocessing import StandardScaler\n\nmfg_data_scaled = mfg_data.copy().drop(columns=['Target'])\n\nscaler = StandardScaler()\nnum_cols = mfg_data_scaled.select_dtypes(include=\"number\").columns.to_list()\n\nmfg_data_scaled[num_cols] = scaler.fit_transform(mfg_data_scaled[num_cols])\nmfg_data_scaled['Target'] = mfg_data['Target']\n\n\nnum_cols = mfg_data_scaled.select_dtypes(include=\"number\").columns.to_list()\nplot_df = mfg_data_scaled[num_cols]\nplt.figure(figsize=(10,6))\nsns.pairplot(plot_df, diag_kind='kde',hue='Target')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\ndfsgsdf=Msdfsdfgdsfhgdsadfgasdfgsdfgsdfgsdfga_3 = mfg_data_scaled.copy()\nmfg_data_3 = mfg_data_3.drop(columns=['UDI']) #High Cardinality\nmfg_data_3 = mfg_data_3.drop(columns=['Product ID']) #High Cardinality\n\nmfg_data_3.head()\n\n\n\n\n\n\n\n\nType\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nFailure Type\nTarget\n\n\n\n\n0\nM\n-0.952389\n-0.947360\n0.068185\n0.282200\n-1.695984\nNo Failure\n0\n\n\n1\nL\n-0.902393\n-0.879959\n-0.729472\n0.633308\n-1.648852\nNo Failure\n0\n\n\n2\nL\n-0.952389\n-1.014761\n-0.227450\n0.944290\n-1.617430\nNo Failure\n0\n\n\n3\nL\n-0.902393\n-0.947360\n-0.590021\n-0.048845\n-1.586009\nNo Failure\n0\n\n\n4\nL\n-0.902393\n-0.879959\n-0.729472\n0.001313\n-1.554588\nNo Failure\n0\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nl_encoder = LabelEncoder()\nmfg_data_3['FailureType_encoded'] = l_encoder.fit_transform(mfg_data_3['Failure Type'])\nmfg_data_3.head()\n\n\n\n\n\n\n\n\nType\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nFailure Type\nTarget\nFailureType_encoded\n\n\n\n\n0\nM\n-0.952389\n-0.947360\n0.068185\n0.282200\n-1.695984\nNo Failure\n0\n1\n\n\n1\nL\n-0.902393\n-0.879959\n-0.729472\n0.633308\n-1.648852\nNo Failure\n0\n1\n\n\n2\nL\n-0.952389\n-1.014761\n-0.227450\n0.944290\n-1.617430\nNo Failure\n0\n1\n\n\n3\nL\n-0.902393\n-0.947360\n-0.590021\n-0.048845\n-1.586009\nNo Failure\n0\n1\n\n\n4\nL\n-0.902393\n-0.879959\n-0.729472\n0.001313\n-1.554588\nNo Failure\n0\n1\n\n\n\n\n\n\n\n\nenconder_mapping = dict(zip(l_encoder.classes_, range(len(l_encoder.classes_))))\n\nprint(enconder_mapping)\n\n{'Heat Dissipation Failure': 0, 'No Failure': 1, 'Overstrain Failure': 2, 'Power Failure': 3, 'Random Failures': 4, 'Tool Wear Failure': 5}\n\n\n\nmfg_data_3 = mfg_data_3.drop(columns=['Failure Type'])\nmfg_data_3 = pd.get_dummies(mfg_data_3)\nmfg_data_3.head()\n\n\n\n\n\n\n\n\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nTarget\nFailureType_encoded\nType_H\nType_L\nType_M\n\n\n\n\n0\n-0.952389\n-0.947360\n0.068185\n0.282200\n-1.695984\n0\n1\nFalse\nFalse\nTrue\n\n\n1\n-0.902393\n-0.879959\n-0.729472\n0.633308\n-1.648852\n0\n1\nFalse\nTrue\nFalse\n\n\n2\n-0.952389\n-1.014761\n-0.227450\n0.944290\n-1.617430\n0\n1\nFalse\nTrue\nFalse\n\n\n3\n-0.902393\n-0.947360\n-0.590021\n-0.048845\n-1.586009\n0\n1\nFalse\nTrue\nFalse\n\n\n4\n-0.902393\n-0.879959\n-0.729472\n0.001313\n-1.554588\n0\n1\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\nmfg_data_3.to_csv('..\\data\\processed\\predictive_maintenance_ready.csv',index=False)"
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "",
    "text": "Uncovering Patterns and Anomalies in Manufacturing Data"
  },
  {
    "objectID": "proposal.html#project-description",
    "href": "proposal.html#project-description",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "",
    "text": "Uncovering Patterns and Anomalies in Manufacturing Data"
  },
  {
    "objectID": "proposal.html#goals",
    "href": "proposal.html#goals",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "🎯Goals:",
    "text": "🎯Goals:\nThe construction of modern factories is resulting in the generation of vast amounts of data. Manufacturing equipment continuously monitors various parameters, such as temperatures, vibrations, motor speeds, and energy consumption, using sensors and other methods. Variations in these parameters can indicate shifts in performance, potentially leading to defects or catastrophic failures in the equipment. Detecting these shifts has become increasingly important to reduce downtime and boost productivity.\nAdvanced techniques such as machine learning, anomaly detection, and image analysis are currently being utilized to forecast when equipment might require maintenance, calibration, or material changes. This project aims to leverage synthetic public data from Kaggle to compare various classification and regression models, with the objective of predicting these critical events. If time allows, we will also explore anomaly detection techniques on time series data to predict potential failures as early as possible.\nSpecific Objectives:\n\nFirst objective is to build a classification model for failures (will compare multiple options). The model will analyze sensor data, such as air temperature, process temperature, rotational speed, and torque, from a predictive maintenance dataset to accurately predict the specific fails and failure type.\nSecond objective is to develop a regression model for anomaly detection and compare to time series analysis (e.g. ARIMA, LSTM). This model will use key features like sensor data and performance metrics to identify unusual patterns."
  },
  {
    "objectID": "proposal.html#proposed-datasets",
    "href": "proposal.html#proposed-datasets",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "📊Proposed Datasets:",
    "text": "📊Proposed Datasets:\n\nSource: Kaggle - Machine Predictive Maintenance Classification (Synthetic dataset that reflects real predictive maintenance encountered in the industry)\n\nData Example:\n\n\n\n\n\n\n\n\n\nUDI\nProduct ID\nType\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nTarget\nFailure Type\n\n\n\n\n0\n1\nM14860\nM\n298.1\n308.6\n1551\n42.8\n0\n0\nNo Failure\n\n\n1\n2\nL47181\nL\n298.2\n308.7\n1408\n46.3\n3\n0\nNo Failure\n\n\n2\n3\nL47182\nL\n298.1\n308.5\n1498\n49.4\n5\n0\nNo Failure\n\n\n3\n4\nL47183\nL\n298.2\n308.6\n1433\n39.5\n7\n0\nNo Failure\n\n\n4\n5\nL47184\nL\n298.2\n308.7\n1408\n40.0\n9\n0\nNo Failure\n\n\n\n\n\n\n\n\nExample of data, 3 categorical features and 6 numerical features to be used. This dataset will be used for classification models.\n\n\n\n\n\n\n\n\n\n\nUDI\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nTarget\n\n\n\n\ncount\n10000.00000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n5000.50000\n300.004930\n310.005560\n1538.776100\n39.986910\n107.951000\n0.033900\n\n\nstd\n2886.89568\n2.000259\n1.483734\n179.284096\n9.968934\n63.654147\n0.180981\n\n\nmin\n1.00000\n295.300000\n305.700000\n1168.000000\n3.800000\n0.000000\n0.000000\n\n\n25%\n2500.75000\n298.300000\n308.800000\n1423.000000\n33.200000\n53.000000\n0.000000\n\n\n50%\n5000.50000\n300.100000\n310.100000\n1503.000000\n40.100000\n108.000000\n0.000000\n\n\n75%\n7500.25000\n301.500000\n311.100000\n1612.000000\n46.800000\n162.000000\n0.000000\n\n\nmax\n10000.00000\n304.500000\n313.800000\n2886.000000\n76.600000\n253.000000\n1.000000\n\n\n\n\n\n\n\n\nThere are 10000 rows on the predictive maintenance dataset, max values for Rotational speed and Tool wear might indacate outliers or some level of skewness on the data. This will be handle during the data preparation part of the project.\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 10 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   UDI                      10000 non-null  int64  \n 1   Product ID               10000 non-null  object \n 2   Type                     10000 non-null  object \n 3   Air temperature [K]      10000 non-null  float64\n 4   Process temperature [K]  10000 non-null  float64\n 5   Rotational speed [rpm]   10000 non-null  int64  \n 6   Torque [Nm]              10000 non-null  float64\n 7   Tool wear [min]          10000 non-null  int64  \n 8   Target                   10000 non-null  int64  \n 9   Failure Type             10000 non-null  object \ndtypes: float64(3), int64(4), object(3)\nmemory usage: 781.4+ KB\n\n\n\nThere are no missing values on this dataset\n\n\nSource: Kaggle - Intelligent Manufacturing Dataset (The Intelligent Manufacturing Dataset for Predictive Optimization is a dataset designed for research in smart manufacturing, AI-driven process optimization, and predictive maintenance)\n\nData Example:\n\n\n\n\n\n\n\n\n\nTimestamp\nMachine_ID\nOperation_Mode\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_%\nQuality_Control_Defect_Rate_%\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_%\nEfficiency_Status\n\n\n\n\n0\n2024-01-01 00:00:00\n39\nIdle\n74.137590\n3.500595\n8.612162\n10.650542\n0.207764\n7.751261\n477.657391\n0.344650\n14.965470\nLow\n\n\n1\n2024-01-01 00:01:00\n29\nActive\n84.264558\n3.355928\n2.268559\n29.111810\n2.228464\n4.989172\n398.174747\n0.769848\n7.678270\nLow\n\n\n2\n2024-01-01 00:02:00\n15\nActive\n44.280102\n2.079766\n6.144105\n18.357292\n1.639416\n0.456816\n108.074959\n0.987086\n8.198391\nLow\n\n\n3\n2024-01-01 00:03:00\n43\nActive\n40.568502\n0.298238\n4.067825\n29.153629\n1.161021\n4.582974\n329.579410\n0.983390\n2.740847\nMedium\n\n\n4\n2024-01-01 00:04:00\n8\nIdle\n75.063817\n0.345810\n6.225737\n34.029191\n4.796520\n2.287716\n159.113525\n0.573117\n12.100686\nLow\n\n\n\n\n\n\n\n\nExample of dataset, dates, numerical and categorical variables. This dataset will be used to explore regression models, time series analysis and anomaly detection.\n\n\n\n\n\n\n\n\n\n\nMachine_ID\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_%\nQuality_Control_Defect_Rate_%\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_%\n\n\n\n\ncount\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n\n\nmean\n25.499330\n60.041458\n2.549959\n5.745929\n25.555620\n2.493418\n5.008806\n275.916324\n0.499385\n7.504100\n\n\nstd\n14.389439\n17.323238\n1.414127\n2.451271\n14.120758\n1.443273\n2.883666\n130.096892\n0.288814\n4.335896\n\n\nmin\n1.000000\n30.000138\n0.100011\n1.500183\n1.000025\n0.000026\n0.000449\n50.000375\n0.000003\n0.000112\n\n\n25%\n13.000000\n45.031596\n1.323214\n3.627318\n13.355118\n1.245026\n2.521591\n162.873618\n0.248166\n3.750148\n\n\n50%\n25.000000\n60.033597\n2.549441\n5.755460\n25.536079\n2.487667\n5.003569\n276.648922\n0.499209\n7.504145\n\n\n75%\n38.000000\n74.967217\n3.776459\n7.860267\n37.796372\n3.741252\n7.506127\n388.812761\n0.748810\n11.273189\n\n\nmax\n50.000000\n89.998979\n4.999974\n9.999889\n49.999917\n4.999975\n9.999900\n499.996768\n0.999978\n14.999869\n\n\n\n\n\n\n\n\nThere are 100000 rows on this dataset.\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 13 columns):\n #   Column                         Non-Null Count   Dtype  \n---  ------                         --------------   -----  \n 0   Timestamp                      100000 non-null  object \n 1   Machine_ID                     100000 non-null  int64  \n 2   Operation_Mode                 100000 non-null  object \n 3   Temperature_C                  100000 non-null  float64\n 4   Vibration_Hz                   100000 non-null  float64\n 5   Power_Consumption_kW           100000 non-null  float64\n 6   Network_Latency_ms             100000 non-null  float64\n 7   Packet_Loss_%                  100000 non-null  float64\n 8   Quality_Control_Defect_Rate_%  100000 non-null  float64\n 9   Production_Speed_units_per_hr  100000 non-null  float64\n 10  Predictive_Maintenance_Score   100000 non-null  float64\n 11  Error_Rate_%                   100000 non-null  float64\n 12  Efficiency_Status              100000 non-null  object \ndtypes: float64(9), int64(1), object(3)\nmemory usage: 9.9+ MB\n\n\n\nThere are no missing data on any of the features."
  },
  {
    "objectID": "proposal.html#project-schedule",
    "href": "proposal.html#project-schedule",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "🗓️Project Schedule",
    "text": "🗓️Project Schedule\n\nDefinition of problem statement and goals. Due Date: 8/6/2025\nPlan to incorporate peer review feedback into project plan. Due Date: 8/7/2025\nData cleaning. (handling missing, outliers, define imputation methods). Due Date: 8/13/2025\nDefine key response on the datasets, depending on the model (might want to look like defects pass/fail) for a classification model or defect rate for a regression model. Due Date: 8/13/2025\nAnalyze features (use PCA or others to understand which features contribute more the variability, etc.) Due Date: 8/13/2025\nClassification Model Creation and Validation. Due Date: 8/15/2025\nRegression Model Creation and Validation,. Due Date: 8/18/2025\nIncorporate time series analysis and compare models and recommend the best one. . Due Date: 8/18/2025\nPrepare final report and presentation Due Date: 8/20/2025"
  },
  {
    "objectID": "proposal.html#project-organization",
    "href": "proposal.html#project-organization",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "📁Project Organization",
    "text": "📁Project Organization\n| FINAL-PROJECT-CASTRO\n| — 📁DATA: # Raw Data files obtained from Kaggle source in CSV format.\n| ______|—- 📁processed: # Cleaned and processed datasets\n| ______|—- 📁results: # model evaluation and other results\n| — 📁IMAGES: # Any images to be used by quarto site\n| — 📁presentation_files: # Quarto presentation files\n| — 📁extra: # Additional documents or files used on project\n| — 📁quarto: # quarto files\n| — 📁src: # source code used for project\n| — 📁.github: # github configuration files\n| – 📄requirements.txt: # Python Dependencies\n| – 📄_quarto.yml # quarto metadata and configuration\n| – 📄.gitignore # list of files and directories to be ignore by Git\n| – 📄about.qmd # Quarto about page with general information about the project\n| – 📄presentation.qmd # Quarto final project presentation\n| – 📄proposal.qmd # Project Problem statement and proposal\n| – 📄README.md # main read me file for git."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "",
    "text": "In recent decades, industry continues modernizing processes and equipment. This is creating enormous amounts of data that in many cases might be underutilized. Vast and rich data from machines, like temperatures, vibration, and pressure, are constantly monitored. Traditional statistical process control is vastly used to oversee key process parameters and provide feedback to technicians when something is behaving abnormally. In recent years, with the explosion of AI, industry has been looking at different approaches to monitor these parameters and use different techniques to more effectively predict or detect defects in products or problems.\n\n\nThis study focuses on using machine learning algorithms like random forest and gradient boosting to predict failures and the type of failure. Real factory data has noise, interactions with many variables, and is highly imbalanced. Machines are expected to run all the time without failure, and processes ideally will produce products without any defects, which makes data highly skewed toward a good state. Tuning models to handle this imbalance is critical. Different sampling methods were evaluated: creating synthetic data to over-sample the negative, under-sampling the positive, and giving weights are approaches that can be used.\n\n\nA second study was done on time-series data, where algorithms like ARIMA and LSTM were used to detect outliers over time. One big challenge is that manufacturing KPIs are typically centered around a target, and variation is random, ideally following a normal distribution but not necessarily following a pattern, which limits the use of these algorithms to predict results. Results from this approaches can still be used to detect outliers, but might not be the best approach.\n\nPresentation: Panopto 🎥\nThis initial exploration of multiple options resulted in very high ROC_AUC scores for most of the models and not so great results for the F1 score. Based on these results we can observed: models might be over-fitting the data, resulting in high scores, and second, because data is highly imbalanced, model is great at predicting 0s (good) as they represent the vast majority. When looking at the F1 score, the precision and recall for predicting the 1s is not the best. For the purposes of this study, predicting the 1s (fails) is the main purpose in a real industry use case.\nBased on this initial results two models will be further evaluated Random Forest Classifer and XGBoost. Fine tuning hyper-parameters and working multiple methods of sampling to reduce or manage the imbalance of the data.\nFor hyper-parameter tuning a combination of sklearn.model_selection - GridSearchCV and RandomizedSearchCV were used to run over multiple options.\nFinal hyper-parameters for Random Forest Classifier Model:\nResults for Random Forest Classifier Model:\nRandomForestClassifier: ROC AUC on test dataset: 0.9751\nRandomForestClassifier: f1 score on test dataset: 0.6258\nCross validation is used to understand if model is over-fitting:\nCross validation results for Random Forest\nfit_time Cross Validation results 0.21\nscore_time Cross Validation results 0.01\ntest_accuracy Cross Validation results 0.90\ntrain_accuracy Cross Validation results 0.99\ntest_precision Cross Validation results 0.69\ntrain_precision Cross Validation results 0.97\ntest_recall Cross Validation results 0.46\ntrain_recall Cross Validation results 0.72\ntest_f1 Cross Validation results 0.45\ntrain_f1 Cross Validation results 0.83\ntest_roc_auc Cross Validation results 0.90\ntrain_roc_auc Cross Validation results 1.00\nAfter tuning the model, the best F1 score obtained is 0.62. Cross-validation results suggest the model is over-fitting and might not be able to generalize well.\nDifferent techniques were explored to see if the model would improve. First attemp was using Synthetic Minority Oversampling Technique (SMOTE) from the imblearn library, the intent of this method is to over-sample the minority class by creating synthetic data. The results from this were worse than the original model. Additionally, a under-sampling method RandomUnderSampler from the same imblearn library was tested; on this case, it was used to reduce the sample of the majority class and try to balance the data. The results were not better than original tuned model.\nAdditionally, to improve the F1 score, a change in the probability threshold was explored; instead of using the normal 0.5, an analysis was done to estimate the ideal point to optimized the F1-Score.\nFigure 6. Values of Recall, Precision and F1 Score metrics for every threshold.\nAs observed in Figures 7 and 8, after improving the threshold a balance can be found between precision and recall. Depending on the use case, this optimization can be used to improve one of them, in some cases model might need to be tune in a specific direction to reduce over-reject or under-reject.\nA second approach is to use XGBoost; this model has the option to handle weights for each class. By adding a higher weight to the minority class, it is expected to handle the imbalance in the dataset better.\nXGBoost Results:\nF1 Score: 0.757\nCross validation results for Random Forest\nfit_time Cross Validation results 0.07\nscore_time Cross Validation results 0.01\ntest_accuracy Cross Validation results 0.98\ntrain_accuracy Cross Validation results 1.00\ntest_precision Cross Validation results 0.74\ntrain_precision Cross Validation results 1.00\ntest_recall Cross Validation results 0.72\ntrain_recall Cross Validation results 1.00\ntest_f1 Cross Validation results 0.73\ntrain_f1 Cross Validation results 1.00\ntest_roc_auc Cross Validation results 0.97\ntrain_roc_auc Cross Validation results 1.00\nThe F1 score for XGBoost without significant tuning and using weights is better than the random forest model originally used. Cross-validation results are still showing some level of over-fitting but improved from the original model. Hyperparameter tuning was done with a similar approach as with random forest, but no significant improvement was observed.\nFigure 9. Confusion Matrix for results of XGBoost Model.\nXGboost results are better to the random forest optimized threshold model. Based on the cross validation results over-fitting seems also slightly better for the XGBoost model."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "",
    "text": "In recent decades, industry continues modernizing processes and equipment. This is creating enormous amounts of data that in many cases might be underutilized. Vast and rich data from machines, like temperatures, vibration, and pressure, are constantly monitored. Traditional statistical process control is vastly used to oversee key process parameters and provide feedback to technicians when something is behaving abnormally. In recent years, with the explosion of AI, industry has been looking at different approaches to monitor these parameters and use different techniques to more effectively predict or detect defects in products or problems.\n\n\nThis study focuses on using machine learning algorithms like random forest and gradient boosting to predict failures and the type of failure. Real factory data has noise, interactions with many variables, and is highly imbalanced. Machines are expected to run all the time without failure, and processes ideally will produce products without any defects, which makes data highly skewed toward a good state. Tuning models to handle this imbalance is critical. Different sampling methods were evaluated: creating synthetic data to over-sample the negative, under-sampling the positive, and giving weights are approaches that can be used.\n\n\nA second study was done on time-series data, where algorithms like ARIMA and LSTM were used to detect outliers over time. One big challenge is that manufacturing KPIs are typically centered around a target, and variation is random, ideally following a normal distribution but not necessarily following a pattern, which limits the use of these algorithms to predict results. Results from this approaches can still be used to detect outliers, but might not be the best approach.\n\nPresentation: Panopto 🎥"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Background",
    "text": "Background\n\nTo explore machine algorithms to detect fails, a dataset from Kaggle was used. The data “Machine Predictive Maintenance Classification is a synthetic dataset that reflects a real use case in the industry, based on the source. The dataset consists of 10000 rows with 14 different features.\n\nKey Features: (Source: https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification)\n\nUID: unique identifier ranging from 1 to 10000\nproductID: consisting of a letter L, M, or H for low (50% of all products), medium (30%), and high (20%) as product quality variants and a variant-specific serial number\nType: is a columns that consist only on the letters L, M and H from productID.\nair temperature [K]: generated using a random walk process later normalized to a standard deviation of 2 K around 300 K\nprocess temperature [K]: generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.\nrotational speed [rpm]: calculated from powepower of 2860 W, overlaid with a normally distributed noise\ntorque [Nm]: torque values are normally distributed around 40 Nm with an Ïƒ = 10 Nm and no negative values.\ntool wear [min]: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process.\nMachine failure: label that indicates, whether the machine has failed in this particular data point for any of the following failure modes are true\n\n\n\n\n\n\n\n\n\n\nUDI\nProduct ID\nType\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nTarget\nFailure Type\n\n\n\n\n0\n1\nM14860\nM\n298.1\n308.6\n1551\n42.8\n0\n0\nNo Failure\n\n\n1\n2\nL47181\nL\n298.2\n308.7\n1408\n46.3\n3\n0\nNo Failure\n\n\n2\n3\nL47182\nL\n298.1\n308.5\n1498\n49.4\n5\n0\nNo Failure\n\n\n3\n4\nL47183\nL\n298.2\n308.6\n1433\n39.5\n7\n0\nNo Failure\n\n\n4\n5\nL47184\nL\n298.2\n308.7\n1408\n40.0\n9\n0\nNo Failure\n\n\n\n\n\n\n\nTable1: Example of 5 rows of the synthetic data used for predictive modeling.\n\n\n\n\n\n\n\n\n\nProcess temperature [K] Mean: 310.01\nProcess temperature [K] Median: 310.10\nProcess temperature [K] Max: 313.80\nProcess temperature [K] Min: 305.70\nProcess temperature [K] Standard Deviation: 1.48\nProcess temperature [K] Number of Points: 10000\n\n\nFigure 1: Example of the distribution observed for 1 synthetic parameters (Process Temperature). Visually data seems not having a significant skew, relatively close to a normal distribution.\n\n\n\n\n\n\n\n\n\nRotational speed [rpm] Mean: 1538.78\nRotational speed [rpm] Median: 1503.00\nRotational speed [rpm] Max: 2886.00\nRotational speed [rpm] Min: 1168.00\nRotational speed [rpm] Standard Deviation: 179.28\nRotational speed [rpm] Number of Points: 10000\n\n\nFigure 2: Example of the distribution observed for 1 synthetic parameters (Rotational speed [rpm]). On this case data is slightly skewed which in some cases is expected for real machine data.\n\nTable 1 and Figure 1 are examples of what the data looks like; there are no missing data in this dataset as it was synthetically created, which is not normal in a real scenario. Figure 2 shows another parameter where the data are skewed. After observing all parameters, the dataset is a good representation for exploring machine learning algorithms and is able to support conclusions from the analysis.\n\n\nData was standardized using sklearn standardscaler and categorical features encoded before moving to training step.\n\nAll detailed data cleaning and exploration can be found here: https://github.com/INFO-523-SU25/final-project-castro/blob/main/src/Data_Exploration_PDM.ipynb\n\nThe second dataset consists of a simulated real-time sensor data from industrial machines. Source is also from Kaggle and it can be found here: https://www.kaggle.com/datasets/ziya07/intelligent-manufacturing-dataset/data\n\nKey Features:\n\nIndustrial IoT Sensor Data\n\nTemperature_C, Vibration_Hz, Power_Consumption_kW,\n\nNetwork Performance:\n\nNetwork_Latency_ms, Packet_Loss_%, Quality_Control_Defect_Rate_%\n\nProduction Indicators:\n\nProduction_Speed_units_per_hr, Predictive_Maintenance_Score, Error_Rate_%\n\nTarget Column Efficiency_Status\n\n\n\n\n\n\n\n\n\n\nTimestamp\nMachine_ID\nOperation_Mode\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_%\nQuality_Control_Defect_Rate_%\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_%\nEfficiency_Status\n\n\n\n\n0\n2024-01-01 00:00:00\n39\nIdle\n74.137590\n3.500595\n8.612162\n10.650542\n0.207764\n7.751261\n477.657391\n0.344650\n14.965470\nLow\n\n\n1\n2024-01-01 00:01:00\n29\nActive\n84.264558\n3.355928\n2.268559\n29.111810\n2.228464\n4.989172\n398.174747\n0.769848\n7.678270\nLow\n\n\n2\n2024-01-01 00:02:00\n15\nActive\n44.280102\n2.079766\n6.144105\n18.357292\n1.639416\n0.456816\n108.074959\n0.987086\n8.198391\nLow\n\n\n3\n2024-01-01 00:03:00\n43\nActive\n40.568502\n0.298238\n4.067825\n29.153629\n1.161021\n4.582974\n329.579410\n0.983390\n2.740847\nMedium\n\n\n4\n2024-01-01 00:04:00\n8\nIdle\n75.063817\n0.345810\n6.225737\n34.029191\n4.796520\n2.287716\n159.113525\n0.573117\n12.100686\nLow\n\n\n\n\n\n\n\nTable 2: Example of 5 rows of the synthetic data that will be used for time series analysis.\n\n\n\n\n\n\n\n\n\nPower_Consumption_kW Mean: 5.75\nPower_Consumption_kW Median: 5.76\nPower_Consumption_kW Max: 10.00\nPower_Consumption_kW Min: 1.50\nPower_Consumption_kW Standard Deviation: 2.45\nPower_Consumption_kW Number of Points: 100000\n\n\nFigure 3: Example of the distribution observed on second dataset for parameter Power_Consumption_kW. Data does not follow a specific distribution, it seems randomly created over a specific range.\n\nAs can be observed in Figure 3, the data from the second dataset seem more randomly created without following a specific distribution. Data in a real scenario for a machine typically follow some type of distribution and is not completely random; a common situation is that processes typically have a target value or values over time, and there is some natural variation around them. The raw data as they are from this source are not usable for the purpose of this study. In order to make the data more like a real scenario, a mean was calculated every 12 hours for each machine.\n\n\nResults from the data transformation can be observed in figures 4 and 5.\n\n\n\n\n\n\n\n\n\n\nPower_Consumption_kW Mean: 5.74\nPower_Consumption_kW Median: 5.75\nPower_Consumption_kW Max: 8.29\nPower_Consumption_kW Min: 2.96\nPower_Consumption_kW Standard Deviation: 0.67\nPower_Consumption_kW Number of Points: 6950\n\n\nFigure 4: Example of the distribution after transformation.\n\n\n\n\n\n\n\n\n\nFigure 5: Example trend for Power_Consumption_kW for one of the machines.\nAll detailed data cleaning and exploration for the second dataset can be found here: https://github.com/INFO-523-SU25/final-project-castro/blob/main/src/Data_Exploration_MFG6G.ipynb"
  },
  {
    "objectID": "index.html#model-training",
    "href": "index.html#model-training",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Model Training",
    "text": "Model Training\n\nThe first objective of this study is to build a classification model for failures. The model will analyze data from Table 1 to accurately predict the specific failures and failure types.\n\n\nThe initial model will be focused on predicting the Target feature as a binary classification, basically pass or fail based on the dataset. The data were split using 70% for training and 30% for testing using stratification to maintain the distribution of 0s/1s for each set, as the data is highly imbalanced."
  },
  {
    "objectID": "index.html#roc-auc-results",
    "href": "index.html#roc-auc-results",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "ROC-AUC Results",
    "text": "ROC-AUC Results\n\n\n\n\n\n\n\n\n\nname\nROC_AUC\n\n\n\n\n0\nNearest_Neighbors\n0.784725\n\n\n1\nGradient_Boosting\n0.821026\n\n\n2\nDecision_Tree\n0.932831\n\n\n3\nExtra_Trees\n0.909882\n\n\n4\nRandom_Forest\n0.972481\n\n\n5\nNeural_Net\n0.918822\n\n\n6\nAdaBoost\n0.899753\n\n\n7\nNaive_Bayes\n0.827014\n\n\n8\nQDA\n0.857405\n\n\n9\nLogisticRegression\n0.880628\n\n\n\n\n\n\n\nTable 4. ROC AUC results for multiple models evaluated"
  },
  {
    "objectID": "index.html#f1-score-results",
    "href": "index.html#f1-score-results",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "F1 Score Results",
    "text": "F1 Score Results\n\n\n\n\n\n\n\n\n\nname\nf1 score\n\n\n\n\n0\nNearest_Neighbors\n0.423077\n\n\n1\nGradient_Boosting\n0.513966\n\n\n2\nDecision_Tree\n0.503145\n\n\n3\nExtra_Trees\n0.347826\n\n\n4\nRandom_Forest\n0.567742\n\n\n5\nNeural_Net\n0.227642\n\n\n6\nAdaBoost\n0.473373\n\n\n7\nNaive_Bayes\n0.198758\n\n\n8\nQDA\n0.386740\n\n\n9\nLogisticRegression\n0.224000\n\n\n\n\n\n\n\nTable 5. F1 Score for multiple models evaluated"
  },
  {
    "objectID": "index.html#conclusions",
    "href": "index.html#conclusions",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Conclusions",
    "text": "Conclusions\n\nThe study demonstrated the application of concepts in machine learning to common manufacturing problems.\nThe Random Forest Classifier model ROC-AUC scores are high, indicating the model can differentiate effectively between fails and no-fails. However, in a real manufacturing process, the majority of the results are positive/pass, making this indicator not the best for this case. Recall and precision are more appropriate; depending on the use case, we would want to tune the model in one or the other direction or use the F1-score to optimize both.\nThe Random Forest model performed poorly for the F1 score when using a standard threshold of 0.5. The study demonstrated this can be improved by selecting an optimized threshold based on the model results.\nSampling can be a useful method to handle imbalanced datasets; however, in this specific case, it did not provide a significant improvement in model performance.\nAssigning weights to each classes to handle the imbalance sample in combination with gradient boosting (XGboost) model, resulted in better results for F1-Score.\nMulti-class classification results using the learnings from the binary-classification were demonstrated. Due to the nature and frequency of the failures and their relationship with the input features, two classes had zero F1-scores, meaning the model was not able to predict them. Other classes had acceptable F1-scores.\nThe study demonstrated how seasonal-decomposition and ARIMA can be used for anomaly detection in a real manufacturing use case; results showed how data points deviating the most from the center/target can be identified by using these methods. These methods might not be the best for a process where there are no patterns and data might just have random variability from the target."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "References",
    "text": "References\n\nShivam Bansal. “Machine Predictive Maintenance Classification Dataset.” Kaggle. Available at: https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification.\nZiya. “Intelligent Manufacturing Dataset.” Kaggle. Available at: https://www.kaggle.com/datasets/ziya07/intelligent-manufacturing-dataset/data.\nINFO-523 University of Arizona. “Comparing Classifiers.” GitHub Notebook. Available at: https://github.com/dataprofessor/code/blob/master/python/comparing-classifiers.ipynb.\nL. Lemaître, A. Nogueira, and C. K. Aridas. “SMOTE: Synthetic Minority Over-sampling Technique.” In: imbalanced-learn documentation. Available at: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html.\nL. Lemaître, A. Nogueira, and C. K. Aridas. “RandomUnderSampler.” In: imbalanced-learn documentation. Available at: https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html.\nT. Chen and C. Guestrin. “XGBoost for Imbalanced Classification.” XGBoosting.com. Available at: https://xgboosting.com/xgboost-for-imbalanced-classification.\nS. Puranik. “Calinski–Harabasz Index for K-Means Clustering Evaluation.” Towards Data Science, August 2021. Available at: https://towardsdatascience.com/calinski-harabasz-index-for-k-means-clustering-evaluation-using-python-4fefeeb2988e/.\nSkipper Seabold and Josef Perktold. “seasonal_decompose — Seasonal Decomposition of Time Series.” statsmodels, accessed 2025. Available at: https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html.\nJason Brownlee. “Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras.” Machine Learning Mastery, March 10, 2018. Available at: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/.\nExplanations, troubleshooting, grammar and clarifications were aided by ChatGPT (OpenAI, 2025). OpenAI. [Large language model]. https://chat.openai.com/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by Cesar Castro M. For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nCesar Castro M. : Second Semester - Master in Data Science - University of Arizona."
  },
  {
    "objectID": "presentation.html#introduction",
    "href": "presentation.html#introduction",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Introduction",
    "text": "Introduction\n\nAll code used to generate the charts and tables on this presentation can be found here:\nhttps://github.com/INFO-523-SU25/final-project-castro/tree/main/src"
  },
  {
    "objectID": "presentation.html#dataset-for-machine-learning-classification",
    "href": "presentation.html#dataset-for-machine-learning-classification",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Dataset for Machine Learning Classification",
    "text": "Dataset for Machine Learning Classification\n\nSource: Machine Predictive Maintenance Classification\nKey Features:\n\n\nUID: unique identifier ranging from 1 to 10000\nproductID: consisting of a letter L, M, or H for low (50% of all products), medium (30%), and high (20%) as product quality variants and a variant-specific serial number\nType: is a columns that consist only on the letters L, M and H from productID.\nair temperature [K]: generated using a random walk process later normalized to a standard deviation of 2 K around 300 K\nprocess temperature [K]: generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.\nrotational speed [rpm]: calculated from powepower of 2860 W, overlaid with a normally distributed noise\ntorque [Nm]: torque values are normally distributed around 40 Nm with an Ïƒ = 10 Nm and no negative values.\ntool wear [min]: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process.\nMachine failure: label that indicates, whether the machine has failed in this particular data point for any of the following failure modes are true"
  },
  {
    "objectID": "presentation.html#manufacturing-data-exploration",
    "href": "presentation.html#manufacturing-data-exploration",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Manufacturing Data Exploration",
    "text": "Manufacturing Data Exploration\nExample of 5 rows of the synthetic data used for classification problem.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUDI\n\n\n\nProduct ID\n\n\n\nType\n\n\n\nAir temperature [K]\n\n\n\nProcess temperature [K]\n\n\n\nRotational speed [rpm]\n\n\n\nTorque [Nm]\n\n\n\nTool wear [min]\n\n\n\nTarget\n\n\n\nFailure Type\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n1\n\n\n\nM14860\n\n\n\nM\n\n\n\n298.1\n\n\n\n308.6\n\n\n\n1551\n\n\n\n42.8\n\n\n\n0\n\n\n\n0\n\n\n\nNo Failure\n\n\n\n\n\n\n\n1\n\n\n\n2\n\n\n\nL47181\n\n\n\nL\n\n\n\n298.2\n\n\n\n308.7\n\n\n\n1408\n\n\n\n46.3\n\n\n\n3\n\n\n\n0\n\n\n\nNo Failure\n\n\n\n\n\n\n\n2\n\n\n\n3\n\n\n\nL47182\n\n\n\nL\n\n\n\n298.1\n\n\n\n308.5\n\n\n\n1498\n\n\n\n49.4\n\n\n\n5\n\n\n\n0\n\n\n\nNo Failure\n\n\n\n\n\n\n\n3\n\n\n\n4\n\n\n\nL47183\n\n\n\nL\n\n\n\n298.2\n\n\n\n308.6\n\n\n\n1433\n\n\n\n39.5\n\n\n\n7\n\n\n\n0\n\n\n\nNo Failure\n\n\n\n\n\n\n\n4\n\n\n\n5\n\n\n\nL47184\n\n\n\nL\n\n\n\n298.2\n\n\n\n308.7\n\n\n\n1408\n\n\n\n40.0\n\n\n\n9\n\n\n\n0\n\n\n\nNo Failure"
  },
  {
    "objectID": "presentation.html#manufacturing-data-exploration-1",
    "href": "presentation.html#manufacturing-data-exploration-1",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Manufacturing Data Exploration",
    "text": "Manufacturing Data Exploration\n\nNumerical Features Relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFailures marked on the pair plot showing stronger relationship with some features.\nThere is some non-linear correlation observed between Torque and Rotation speed (suggesting data is not entirely random and mimic a real use case).\nThere is a clear relationship between the fails and some of the features."
  },
  {
    "objectID": "presentation.html#manufacturing-data-preparation",
    "href": "presentation.html#manufacturing-data-preparation",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Manufacturing Data Preparation",
    "text": "Manufacturing Data Preparation\nData Scaling\n#Standardization\nfrom sklearn.preprocessing import StandardScaler # use StandardScalar from sklearn\n\nmfg_data_scaled = mfg_data.copy().drop(columns=['Target'])\n\nscaler = StandardScaler()\nnum_cols = mfg_data_scaled.select_dtypes(include=\"number\").columns.to_list()\nmfg_data_scaled[num_cols] = scaler.fit_transform(mfg_data_scaled[num_cols])\nCategorical Features Encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nl_encoder = LabelEncoder()\nmfg_data_3['FailureType_encoded'] = l_encoder.fit_transform(mfg_data_3['Failure Type'])"
  },
  {
    "objectID": "presentation.html#model-selection",
    "href": "presentation.html#model-selection",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Model Selection",
    "text": "Model Selection\n\nMethod adapted from Chanin Nantasenamat’s GitHub code, multiple classification models were compared for ROC-AUC and the F1 score."
  },
  {
    "objectID": "presentation.html#model-selection-cont.",
    "href": "presentation.html#model-selection-cont.",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Model Selection Cont.",
    "text": "Model Selection Cont.\n\nTwo models will be evaluated Random Forest Classifer based on the results of the comparison and XGboost based on research model might be proper for imbalance datasets (https://xgboosting.com/xgboost-for-imbalanced-classification/)"
  },
  {
    "objectID": "presentation.html#random-forest-model-tuning",
    "href": "presentation.html#random-forest-model-tuning",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Random Forest Model Tuning",
    "text": "Random Forest Model Tuning\n\nFor hyper-parameter tuning a combination of sklearn.model_selection - GridSearchCV and RandomizedSearchCV was used to run over multiple options.\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\nparam_dist = {\n    'n_estimators': randint(50, 500),\n    'max_depth': randint(1, 20),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 20),\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\nrf = RandomForestClassifier()\nrand_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=20, cv=5,scoring='f1')\nrand_search.fit(X_train, y_train)\nprint('Best hyperparameters:', rand_search.best_params_)"
  },
  {
    "objectID": "presentation.html#random-forest-results",
    "href": "presentation.html#random-forest-results",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Random Forest Results",
    "text": "Random Forest Results\n\nAfter tuning the model, the best F1 score obtained is 0.62. Cross-validation results suggest the model is over-fitting and might not be able to generalize.\n\nRandomForestClassifier: ROC AUC on test dataset: 0.9751\nRandomForestClassifier: f1 score on test dataset: 0.6258\n\nCross Validation Results\n\nTest_f1 Cross Validation results 0.45\nTrain_f1 Cross Validation results 0.83\n\nThere is a big difference in performance between test and train results for the CV test."
  },
  {
    "objectID": "presentation.html#handling-imbalanced-dataset",
    "href": "presentation.html#handling-imbalanced-dataset",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Handling imbalanced dataset",
    "text": "Handling imbalanced dataset\n\nDifferent techniques were explored to see if the model would improve over-fitting and F1-score.\n\nOver sampling using Synthetic Minority Oversampling Technique (SMOTE) (imblearn)\nUnder sampling using RandomUnderSampler (imblearn)\n\nNo significant Improvement was observed on this specific dataset."
  },
  {
    "objectID": "presentation.html#random-forest-threshold-optimization",
    "href": "presentation.html#random-forest-threshold-optimization",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Random Forest Threshold Optimization",
    "text": "Random Forest Threshold Optimization\n\nTo improve the F1 score, a change in the probability threshold was explored; instead of using the standard 0.5, an analysis was done to estimate the ideal point to optimized the F1-Score"
  },
  {
    "objectID": "presentation.html#optimized-model-results",
    "href": "presentation.html#optimized-model-results",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Optimized Model Results",
    "text": "Optimized Model Results\n\n\n\nAfter improving the threshold based on the model results, a balance can be found between precision and recall."
  },
  {
    "objectID": "presentation.html#xgboost-model-definition",
    "href": "presentation.html#xgboost-model-definition",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "XGBoost Model Definition",
    "text": "XGBoost Model Definition\n\nA second approach is to employ XGBoost, a model that offers native functionality for applying class-specific weights. By increasing the weight of the minority class, the model is anticipated to more effectively mitigate class imbalance.\n\nimport xgboost as xgb\n\nscale_pos_weight = (len(y0) - np.sum(y0)) / np.sum(y0) #Intent is to give more weight to the minority class (1s on this case)\n\nxgmodel = xgb.XGBClassifier(\n    scale_pos_weight=scale_pos_weight,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=42\n)\n\nxgmodel.fit(X_train, y_train)\ny_pred = xgmodel.predict(X_test)\n\nf1 = f1_score(y_test, y_pred)\nprint(f\"F1 Score: {f1:.3f}\")"
  },
  {
    "objectID": "presentation.html#xgboost-model-results",
    "href": "presentation.html#xgboost-model-results",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "XGBoost Model Results",
    "text": "XGBoost Model Results\n\nThe F1 score for XGBoost without significant tuning and using class-weights is better than the random forest model originally used.\n\n\n\nModel Results\n\nF1 Score: 0.757\n\nCross Validation\n\nTest_f1 Cross Validation results 0.73\nTrain_f1 Cross Validation results 1.00"
  },
  {
    "objectID": "presentation.html#xgboost-multi-class-model",
    "href": "presentation.html#xgboost-multi-class-model",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "XGBoost Multi-Class Model",
    "text": "XGBoost Multi-Class Model\n\nXGBoost results were better than Random Forest. Multi-class model will be created to predict the failure types:\n\n# Defining weights for each class.\nclasses, counts = np.unique(y_train, return_counts=True)\ninv_freq = 1.0 / counts\nclass_weights = dict(zip(classes, inv_freq))\nsample_weights = np.array([class_weights[label] for label in y_train])"
  },
  {
    "objectID": "presentation.html#xgboost-model-results-1",
    "href": "presentation.html#xgboost-model-results-1",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "XGBoost Model Results",
    "text": "XGBoost Model Results\n\n\n\nModel Results\n\nF1 score per class:\n\nHeat-Dissipation: 0.95\nNo Failure: 0.99\nOverstrain Failure 0.76\nPowe Failure: 0.8\nRandom Failures and Tool Wear Failure are both 0."
  },
  {
    "objectID": "presentation.html#xgboost-features-of-importance",
    "href": "presentation.html#xgboost-features-of-importance",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "XGBoost Features of Importance",
    "text": "XGBoost Features of Importance\n\nAlthough the model performance is not great for all classes, understanding what features are important in the prediction model can help context experts interpret results and take action to reduce failures and improve the process overall."
  },
  {
    "objectID": "presentation.html#time-series-analysis",
    "href": "presentation.html#time-series-analysis",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\n\nThe second dataset consists of a simulated real-time sensor data from industrial machines. Source is also from Kaggle and it can be found here: https://www.kaggle.com/datasets/ziya07/intelligent-manufacturing-dataset/data\n\nKey Features:\n\n-   Industrial IoT Sensor Data\n    -   Temperature_C, Vibration_Hz, Power_Consumption_kW,\n-   Network Performance:\n    -   Network_Latency_ms, Packet_Loss_%, Quality_Control_Defect_Rate_%\n-   Production Indicators:\n    -   Production_Speed_units_per_hr, Predictive_Maintenance_Score, Error_Rate_%\n-   Target Column Efficiency_StatusKey Features:"
  },
  {
    "objectID": "presentation.html#time-serie-data-exploration",
    "href": "presentation.html#time-serie-data-exploration",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Serie Data Exploration",
    "text": "Time Serie Data Exploration\n\nExample of 5 rows of the synthetic data used for time series analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimestamp\n\n\n\nMachine_ID\n\n\n\nOperation_Mode\n\n\n\nTemperature_C\n\n\n\nVibration_Hz\n\n\n\nPower_Consumption_kW\n\n\n\nNetwork_Latency_ms\n\n\n\nPacket_Loss_%\n\n\n\nQuality_Control_Defect_Rate_%\n\n\n\nProduction_Speed_units_per_hr\n\n\n\nPredictive_Maintenance_Score\n\n\n\nError_Rate_%\n\n\n\nEfficiency_Status\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n2024-01-01 00:00:00\n\n\n\n39\n\n\n\nIdle\n\n\n\n74.137590\n\n\n\n3.500595\n\n\n\n8.612162\n\n\n\n10.650542\n\n\n\n0.207764\n\n\n\n7.751261\n\n\n\n477.657391\n\n\n\n0.344650\n\n\n\n14.965470\n\n\n\nLow\n\n\n\n\n\n\n\n1\n\n\n\n2024-01-01 00:01:00\n\n\n\n29\n\n\n\nActive\n\n\n\n84.264558\n\n\n\n3.355928\n\n\n\n2.268559\n\n\n\n29.111810\n\n\n\n2.228464\n\n\n\n4.989172\n\n\n\n398.174747\n\n\n\n0.769848\n\n\n\n7.678270\n\n\n\nLow\n\n\n\n\n\n\n\n2\n\n\n\n2024-01-01 00:02:00\n\n\n\n15\n\n\n\nActive\n\n\n\n44.280102\n\n\n\n2.079766\n\n\n\n6.144105\n\n\n\n18.357292\n\n\n\n1.639416\n\n\n\n0.456816\n\n\n\n108.074959\n\n\n\n0.987086\n\n\n\n8.198391\n\n\n\nLow\n\n\n\n\n\n\n\n3\n\n\n\n2024-01-01 00:03:00\n\n\n\n43\n\n\n\nActive\n\n\n\n40.568502\n\n\n\n0.298238\n\n\n\n4.067825\n\n\n\n29.153629\n\n\n\n1.161021\n\n\n\n4.582974\n\n\n\n329.579410\n\n\n\n0.983390\n\n\n\n2.740847\n\n\n\nMedium\n\n\n\n\n\n\n\n4\n\n\n\n2024-01-01 00:04:00\n\n\n\n8\n\n\n\nIdle\n\n\n\n75.063817\n\n\n\n0.345810\n\n\n\n6.225737\n\n\n\n34.029191\n\n\n\n4.796520\n\n\n\n2.287716\n\n\n\n159.113525\n\n\n\n0.573117\n\n\n\n12.100686\n\n\n\nLow"
  },
  {
    "objectID": "presentation.html#time-serie-data-exploration-cont.",
    "href": "presentation.html#time-serie-data-exploration-cont.",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Serie Data Exploration cont.",
    "text": "Time Serie Data Exploration cont.\n\nExample distribution for one Feature\n\n\n\nData seems randomly generated instead of coming from a real scenario."
  },
  {
    "objectID": "presentation.html#time-serie-data-exploration-cont.-1",
    "href": "presentation.html#time-serie-data-exploration-cont.-1",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Serie Data Exploration cont.",
    "text": "Time Serie Data Exploration cont.\n\nIn order to make the data more like a real scenario, a mean was calculated for every 12 hours."
  },
  {
    "objectID": "presentation.html#time-serie-data-exploration-cont.-2",
    "href": "presentation.html#time-serie-data-exploration-cont.-2",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Serie Data Exploration cont.",
    "text": "Time Serie Data Exploration cont.\n\nExample Trend for Power_Consumption_kW for one of the machines."
  },
  {
    "objectID": "presentation.html#time-serie-data-analysis",
    "href": "presentation.html#time-serie-data-analysis",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Serie Data Analysis",
    "text": "Time Serie Data Analysis\n\nAfter pre-processing the data, the study is focused on understanding how algorithms like the seasonal_decompose and ARIMA (or auto_arima) can be used for anomaly detection."
  },
  {
    "objectID": "presentation.html#time-serie-data-seasonal-decomposition",
    "href": "presentation.html#time-serie-data-seasonal-decomposition",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Serie Data Seasonal-Decomposition",
    "text": "Time Serie Data Seasonal-Decomposition\n#https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n# Decompose the time series\ndecomposition = seasonal_decompose(ts_df['Power_Consumption_kW'], model = 'additive',period=14) # 7days for period"
  },
  {
    "objectID": "presentation.html#time-serie-data-seasonal-decomposition-1",
    "href": "presentation.html#time-serie-data-seasonal-decomposition-1",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Serie Data Seasonal-Decomposition",
    "text": "Time Serie Data Seasonal-Decomposition\n\nAnomaly detection using Residuals:\n\n# Anomalies in residuals\nresiduals = decomposition.resid.dropna() #Obtain residuals from decompositions\nthreshold = 2 * residuals.std() # Our rule, on this cases based on research we selected to use 2X the standard deviation of the residuals\nanomalies = np.abs(residuals) &gt; threshold # Applying the rule to obtain the anomalies\n\n4 Data points identified as anomalies using this method"
  },
  {
    "objectID": "presentation.html#time-serie-data-arima",
    "href": "presentation.html#time-serie-data-arima",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Serie Data ARIMA",
    "text": "Time Serie Data ARIMA\n\nModel Fitting\n\n#Fitting auto-arima model\nfrom pmdarima import auto_arima\nauto_model = auto_arima(ts_df['Power_Consumption_kW'], \n                        seasonal=False,\n \n                        error_action = 'ignore',  \n                        suppress_warnings = True, \n                        stepwise = True)\nprint(auto_model.summary())\nprint(f\"Best (p, d, q): ({auto_model.order[0]}, {auto_model.order[1]}, {auto_model.order[2]})\")"
  },
  {
    "objectID": "presentation.html#time-serie-data-arima---results",
    "href": "presentation.html#time-serie-data-arima---results",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Serie Data ARIMA - Results",
    "text": "Time Serie Data ARIMA - Results\n\nResults from Auto-Arima Model Fitting"
  },
  {
    "objectID": "presentation.html#time-serie-data-arima---results-cont.",
    "href": "presentation.html#time-serie-data-arima---results-cont.",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Time Serie Data ARIMA - Results Cont.",
    "text": "Time Serie Data ARIMA - Results Cont.\n\nResults from the predictions are almost a constant value around the center of the distribution, meaning the ARIMA model is closely predicting the mean for every single value of the time series data. One reason for this could be that the data is not predictable."
  },
  {
    "objectID": "presentation.html#conclusions",
    "href": "presentation.html#conclusions",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "Conclusions",
    "text": "Conclusions\n\n\nThe study demonstrated the application of concepts in machine learning to common manufacturing problems.\nThe Random Forest Classifier model ROC-AUC scores are high, indicating the model can differentiate effectively between fails and no-fails. However, in a real manufacturing process, the majority of the results are positive/pass, making this indicator not the best for this case. Recall and precision are more appropriate; depending on the use case, we would want to tune the model in one or the other direction or use the F1-score to optimize both.\nThe Random Forest model performed poorly for the F1 score when using a standard threshold of 0.5. The study demonstrated this can be improved by selecting an optimized threshold based on the model results.\nSampling can be a useful method to handle imbalanced datasets; however, in this specific case, it did not provide a significant improvement in model performance.\nAssigning weights to each classes to handle the imbalance sample in combination with gradient boosting (XGboost) model, resulted in better results for F1-Score.\nMulti-class classification results using the learnings from the binary-classification were demonstrated. Due to the nature and frequency of the failures and their relationship with the input features, two classes had zero F1-scores, meaning the model was not able to predict them. Other classes had acceptable F1-scores.\nThe study demonstrated how seasonal-decomposition and ARIMA can be used for anomaly detection in a real manufacturing use case; results showed how data points deviating the most from the center/target can be identified by using these methods. These methods might not be the best for a process where there are no patterns and data might just have random variability from the target."
  },
  {
    "objectID": "src/Data_Exploration_MFG6G.html",
    "href": "src/Data_Exploration_MFG6G.html",
    "title": "Data Exploration/Cleaning for Manufacturing 6G dataset",
    "section": "",
    "text": "Author: Cesar Castro M\n\n\nDate: 08/11/2025\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\n\n\nmfg_data = pd.read_csv('..\\data\\manufacturing_6G_dataset.csv')\n\n\nmfg_data.head()\n\n\n\n\n\n\n\n\nTimestamp\nMachine_ID\nOperation_Mode\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_%\nQuality_Control_Defect_Rate_%\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_%\nEfficiency_Status\n\n\n\n\n0\n2024-01-01 00:00:00\n39\nIdle\n74.137590\n3.500595\n8.612162\n10.650542\n0.207764\n7.751261\n477.657391\n0.344650\n14.965470\nLow\n\n\n1\n2024-01-01 00:01:00\n29\nActive\n84.264558\n3.355928\n2.268559\n29.111810\n2.228464\n4.989172\n398.174747\n0.769848\n7.678270\nLow\n\n\n2\n2024-01-01 00:02:00\n15\nActive\n44.280102\n2.079766\n6.144105\n18.357292\n1.639416\n0.456816\n108.074959\n0.987086\n8.198391\nLow\n\n\n3\n2024-01-01 00:03:00\n43\nActive\n40.568502\n0.298238\n4.067825\n29.153629\n1.161021\n4.582974\n329.579410\n0.983390\n2.740847\nMedium\n\n\n4\n2024-01-01 00:04:00\n8\nIdle\n75.063817\n0.345810\n6.225737\n34.029191\n4.796520\n2.287716\n159.113525\n0.573117\n12.100686\nLow\n\n\n\n\n\n\n\n\n# Setting right feature type\nmfg_data['Timestamp'] = pd.to_datetime(mfg_data['Timestamp'])\nmfg_data['Machine_ID'] = mfg_data['Machine_ID'].astype('object')\n\n\n#Find Categorical Features on childcare_costs dataset\ndef cat_features_explore(df):\n    def calc_unique_counts(df,col_name):\n        col = df[col_name]\n        unique_val = col.unique()\n        plt.figure(figsize=(12,4))\n        sns.countplot(data=df, x=col_name)\n        plt.title(f'Categories for {col_name}')\n        plt.tight_layout()\n        plt.show()\n        return len(unique_val)\n\n    cat_cols = mfg_data.select_dtypes(include=\"object\").columns.to_list()\n    count_dic = {col:calc_unique_counts(mfg_data,col) for col in cat_cols}\n    print(count_dic)    \n \ncat_features_explore(mfg_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'Machine_ID': 50, 'Operation_Mode': 3, 'Efficiency_Status': 3}\n\n\n\ndef plot_hist_qq(df,col_name):\n    col = df[col_name]\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2,nrows=1,figsize=(8, 4))\n    sns.histplot(col, linewidth=1,ax=ax1)\n    #sns.kdeplot(col, linewidth=5,ax=ax1)\n    ax1.set_title(col_name + ' Histogram')\n\n    sm.qqplot(col,line='s',ax=ax2)\n    ax2.set_title(col_name + ' Q-Q Plot')\n    plt.tight_layout()\n    plt.show()\n\ndef calculate_stats(df,col_name):\n    print (f\"{col_name} Mean: {df[col_name].mean():.2f}\")\n    print (f\"{col_name} Median: {df[col_name].median():.2f}\")\n    print (f\"{col_name} Max: {df[col_name].max():.2f}\")\n    print (f\"{col_name} Min: {df[col_name].min():.2f}\")\n    print (f\"{col_name} Standard Deviation: {df[col_name].std():.2f}\")    \n    print (f\"{col_name} Number of Points: {len(df[col_name]):.0f}\")    \n\nfor col in mfg_data.select_dtypes(include=\"number\").columns.to_list():\n    plot_hist_qq(mfg_data[mfg_data['Machine_ID']==2],col) \n    calculate_stats(mfg_data[mfg_data['Machine_ID']==2],col)\n\n\n\n\n\n\n\n\nTemperature_C Mean: 59.81\nTemperature_C Median: 59.98\nTemperature_C Max: 89.94\nTemperature_C Min: 30.03\nTemperature_C Standard Deviation: 17.64\nTemperature_C Number of Points: 2041\n\n\n\n\n\n\n\n\n\nVibration_Hz Mean: 2.53\nVibration_Hz Median: 2.59\nVibration_Hz Max: 5.00\nVibration_Hz Min: 0.10\nVibration_Hz Standard Deviation: 1.42\nVibration_Hz Number of Points: 2041\n\n\n\n\n\n\n\n\n\nPower_Consumption_kW Mean: 5.72\nPower_Consumption_kW Median: 5.68\nPower_Consumption_kW Max: 10.00\nPower_Consumption_kW Min: 1.50\nPower_Consumption_kW Standard Deviation: 2.46\nPower_Consumption_kW Number of Points: 2041\n\n\n\n\n\n\n\n\n\nNetwork_Latency_ms Mean: 25.45\nNetwork_Latency_ms Median: 25.34\nNetwork_Latency_ms Max: 49.98\nNetwork_Latency_ms Min: 1.02\nNetwork_Latency_ms Standard Deviation: 14.02\nNetwork_Latency_ms Number of Points: 2041\n\n\n\n\n\n\n\n\n\nPacket_Loss_% Mean: 2.52\nPacket_Loss_% Median: 2.56\nPacket_Loss_% Max: 5.00\nPacket_Loss_% Min: 0.01\nPacket_Loss_% Standard Deviation: 1.41\nPacket_Loss_% Number of Points: 2041\n\n\n\n\n\n\n\n\n\nQuality_Control_Defect_Rate_% Mean: 5.01\nQuality_Control_Defect_Rate_% Median: 5.00\nQuality_Control_Defect_Rate_% Max: 9.98\nQuality_Control_Defect_Rate_% Min: 0.00\nQuality_Control_Defect_Rate_% Standard Deviation: 2.90\nQuality_Control_Defect_Rate_% Number of Points: 2041\n\n\n\n\n\n\n\n\n\nProduction_Speed_units_per_hr Mean: 274.51\nProduction_Speed_units_per_hr Median: 270.07\nProduction_Speed_units_per_hr Max: 499.98\nProduction_Speed_units_per_hr Min: 50.35\nProduction_Speed_units_per_hr Standard Deviation: 129.23\nProduction_Speed_units_per_hr Number of Points: 2041\n\n\n\n\n\n\n\n\n\nPredictive_Maintenance_Score Mean: 0.49\nPredictive_Maintenance_Score Median: 0.48\nPredictive_Maintenance_Score Max: 1.00\nPredictive_Maintenance_Score Min: 0.00\nPredictive_Maintenance_Score Standard Deviation: 0.29\nPredictive_Maintenance_Score Number of Points: 2041\n\n\n\n\n\n\n\n\n\nError_Rate_% Mean: 7.52\nError_Rate_% Median: 7.48\nError_Rate_% Max: 15.00\nError_Rate_% Min: 0.00\nError_Rate_% Standard Deviation: 4.38\nError_Rate_% Number of Points: 2041\n\n\n\nmfg_data.select_dtypes(include=\"number\").columns.to_list()\nplt.figure(figsize=(10,6))\nsns.pairplot(mfg_data, diag_kind='kde',hue='Efficiency_Status')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nDATA PREPARATION FOR ANOMALY DETECTION\n\n#mfg_data_2 = mfg_data.groupby(['Machine_ID',mfg_data[\"Timestamp\"].dt.date]).mean(numeric_only=True).reset_index()\n\nmfg_data_2 = (\n    mfg_data\n    .groupby([\n        'Machine_ID', \n        mfg_data[\"Timestamp\"].dt.floor('12H')  # rounds Timestamp down to the hour\n    ])\n    .mean(numeric_only=True)\n    .reset_index()\n)\n\nmfg_data_2['Machine_ID'] = mfg_data_2['Machine_ID'].astype('object')\nmfg_data_2['Timestamp'] = pd.to_datetime(mfg_data_2['Timestamp'])\n\nmfg_data_2.head()\n\nC:\\Users\\cesar\\AppData\\Local\\Temp\\ipykernel_36112\\373264414.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  mfg_data[\"Timestamp\"].dt.floor('12H')  # rounds Timestamp down to the hour\n\n\n\n\n\n\n\n\n\nMachine_ID\nTimestamp\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_%\nQuality_Control_Defect_Rate_%\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_%\n\n\n\n\n0\n1\n2024-01-01 00:00:00\n59.027691\n3.273465\n6.871082\n23.445160\n2.308684\n4.684600\n349.632925\n0.392129\n6.939090\n\n\n1\n1\n2024-01-01 12:00:00\n63.907350\n2.021363\n4.978956\n30.675345\n2.408669\n5.366573\n321.184645\n0.410275\n7.534709\n\n\n2\n1\n2024-01-02 00:00:00\n51.808176\n2.047471\n6.029171\n31.269433\n1.834945\n6.899124\n363.738970\n0.522837\n6.831884\n\n\n3\n1\n2024-01-02 12:00:00\n57.371817\n2.646207\n5.790383\n28.433255\n3.047640\n5.324885\n294.783106\n0.425722\n5.794327\n\n\n4\n1\n2024-01-03 00:00:00\n56.866279\n2.500103\n7.077736\n29.890366\n3.134400\n5.549209\n302.052996\n0.307520\n7.169374\n\n\n\n\n\n\n\n\nmfg_data_2.select_dtypes(include=\"number\").columns.to_list()\nplt.figure(figsize=(10,6))\nsns.pairplot(mfg_data_2, diag_kind='kde',hue='Machine_ID')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nmachine_1 =mfg_data_2[mfg_data_2['Machine_ID']==1] \n\nstart_date = \"2024-01-01\"\nend_date = \"2024-12-30\"\ntred_plot_df = machine_1[(machine_1[\"Timestamp\"] &gt;= start_date) & (machine_1[\"Timestamp\"] &lt;= end_date)]\n\nplt.figure(figsize=(14, 5))\nplt.plot(tred_plot_df[\"Timestamp\"], tred_plot_df[\"Temperature_C\"], marker=\"o\", markersize=3, linestyle=\"-\")\n\nplt.title(\"Temperature (°C) Over Time\", fontsize=14)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Temperature (°C)\")\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n\n\n\n\n\n\n\n\nfor col in mfg_data_2.select_dtypes(include=\"number\").columns.to_list():\n    plot_hist_qq(mfg_data_2,col) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Outliers Overview\ndef report_quantity_outliers(df):\n    for col in df.columns:\n        q25 = df[col].quantile(0.25)\n        q75 = df[col].quantile(0.75)\n        iqr = q75 - q25\n        lower_bound = q25 - 1.5 * iqr\n        upper_bound = q75 + 1.5 * iqr\n        outliers = df[(df[col] &lt; lower_bound) | (df[col] &gt; upper_bound)]\n        print(f\"{col}: {outliers.shape[0]} outliers\")\n\nnum_cols = mfg_data_2.select_dtypes(include=\"number\").columns.to_list()        \n\nreport_quantity_outliers(mfg_data_2[num_cols])\n\nTemperature_C: 72 outliers\nVibration_Hz: 67 outliers\nPower_Consumption_kW: 73 outliers\nNetwork_Latency_ms: 92 outliers\nPacket_Loss_%: 56 outliers\nQuality_Control_Defect_Rate_%: 75 outliers\nProduction_Speed_units_per_hr: 72 outliers\nPredictive_Maintenance_Score: 73 outliers\nError_Rate_%: 77 outliers\n\n\n\nmfg_data_2.to_csv('..\\data\\processed\\mfg_time_series.csv',index=False)"
  },
  {
    "objectID": "src/Model_Training_PDM.html",
    "href": "src/Model_Training_PDM.html",
    "title": "Model Training Defect Detection",
    "section": "",
    "text": "Author: Cesar Castro M\n\n\nDate: 08/11/2025\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, f1_score, precision_score, recall_score\n\nfrom sklearn.model_selection import cross_validate\nimport warnings\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\n\n\nData Preparation\n\n# Read already prepared dataset\nmfg_df = pd.read_csv('..\\data\\processed\\predictive_maintenance_ready.csv')\n\n\n# Replace [] on column names as some libraries have problems with these.\nmfg_df.columns = mfg_df.columns.str.replace('[\\[\\]&lt;&gt;]', '', regex=True).astype(str)\nmfg_df.columns = mfg_df.columns.str.replace(' ', '_').astype(str)\n\nmfg_df.head()\n\n\n\n\n\n\n\n\nAir_temperature_K\nProcess_temperature_K\nRotational_speed_rpm\nTorque_Nm\nTool_wear_min\nTarget\nFailureType_encoded\nType_H\nType_L\nType_M\n\n\n\n\n0\n-0.952389\n-0.947360\n0.068185\n0.282200\n-1.695984\n0\n1\nFalse\nFalse\nTrue\n\n\n1\n-0.902393\n-0.879959\n-0.729472\n0.633308\n-1.648852\n0\n1\nFalse\nTrue\nFalse\n\n\n2\n-0.952389\n-1.014761\n-0.227450\n0.944290\n-1.617430\n0\n1\nFalse\nTrue\nFalse\n\n\n3\n-0.902393\n-0.947360\n-0.590021\n-0.048845\n-1.586009\n0\n1\nFalse\nTrue\nFalse\n\n\n4\n-0.902393\n-0.879959\n-0.729472\n0.001313\n-1.554588\n0\n1\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n# Separate target columns and features\nX =mfg_df.drop(columns=['Target','FailureType_encoded'],axis=1)\ny0 = mfg_df['Target']\ny1 = mfg_df['FailureType_encoded']\n\n\n\nSplit data for training and testing\n\nX_train, X_test, y_train, y_test = train_test_split(X, y0, stratify=y0,  test_size=0.3, random_state=42)\n\n\n#Data is highly imbalanced\nprint(f'Total fails: {y0.sum()} out of total data points {len(y0)}')\n\nTotal fails: 339 out of total data points 10000\n\n\nCommonly manufacturing data for defects is very imbalanced as yields are typically high\n\n\nModel Training and Comparison\n\n# Will Try Multiple Models based on some research\n# Rerference: https://github.com/dataprofessor/code/blob/master/python/comparing-classifiers.ipynb\n\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nnames = [\"Nearest_Neighbors\",  \"Gradient_Boosting\", \"Decision_Tree\",\n        \"Extra_Trees\", \"Random_Forest\", \"Neural_Net\", \"AdaBoost\",\n        \"Naive_Bayes\", \"QDA\",'LogisticRegression']\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0),\n    DecisionTreeClassifier(max_depth=5),\n    ExtraTreesClassifier(n_estimators=10, min_samples_split=2),\n    RandomForestClassifier(random_state=42),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(n_estimators=100),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(random_state=42)\n    ]\n\n\nscores = []\nmetric = roc_auc_score\nfor name, clf in zip(names, classifiers):\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    y_pred = clf.predict(X_test)\n    y_prob = clf.predict_proba(X_test)[:,1]\n    rscore = metric(y_test, y_prob)\n    scores.append(rscore)\n\n#https://github.com/dataprofessor/code/blob/master/python/comparing-classifiers.ipynb\ndf = pd.DataFrame()\ndf['name'] = names\ndf['ROC_AUC'] = scores\ndf\n\n\n\n\n\n\n\n\nname\nROC_AUC\n\n\n\n\n0\nNearest_Neighbors\n0.784725\n\n\n1\nGradient_Boosting\n0.820843\n\n\n2\nDecision_Tree\n0.932831\n\n\n3\nExtra_Trees\n0.906656\n\n\n4\nRandom_Forest\n0.972481\n\n\n5\nNeural_Net\n0.917803\n\n\n6\nAdaBoost\n0.899753\n\n\n7\nNaive_Bayes\n0.827014\n\n\n8\nQDA\n0.838547\n\n\n9\nLogisticRegression\n0.880628\n\n\n\n\n\n\n\nROC AUC Scores are high for majority of the models evaluated\n\nscores = []\nmetric = f1_score\nfor name, clf in zip(names, classifiers):\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    y_pred = clf.predict(X_test)\n    y_prob = clf.predict_proba(X_test)[:,1]\n    rscore = metric(y_test, y_pred)\n    scores.append(rscore)\n\n#https://github.com/dataprofessor/code/blob/master/python/comparing-classifiers.ipynb\ndf = pd.DataFrame()\ndf['name'] = names\ndf['f1 score'] = scores\ndf\n\n\n\n\n\n\n\n\nname\nf1 score\n\n\n\n\n0\nNearest_Neighbors\n0.423077\n\n\n1\nGradient_Boosting\n0.513966\n\n\n2\nDecision_Tree\n0.503145\n\n\n3\nExtra_Trees\n0.422535\n\n\n4\nRandom_Forest\n0.567742\n\n\n5\nNeural_Net\n0.214876\n\n\n6\nAdaBoost\n0.473373\n\n\n7\nNaive_Bayes\n0.198758\n\n\n8\nQDA\n0.278788\n\n\n9\nLogisticRegression\n0.224000\n\n\n\n\n\n\n\nF1 Score on the other side are not as high, indicating models can predict one class (the majority) but not great when predicting the target/fails.\n\n\nHyperparemeter Tunning for Random Forest\nMethod #1: using GridSearchCV\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Basic Model\nrf = RandomForestClassifier(random_state=42)\n\n# Random Parameters\nparam_grid = {\n    'n_estimators': [50, 250, 500],\n    'max_depth': [5, 10, 20],\n    'min_samples_split': [5, 15, 30],\n    'min_samples_leaf': [5, 10, 30],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\n\ngrid_search = GridSearchCV(\n    estimator=rf,\n    param_grid=param_grid,\n    cv=5,                   # 5-fold cross-validation\n    scoring='f1',           # Focus on F1 score as trying to improve for the target 1 prediction.   \n    n_jobs=-1,              \n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train)\n\n# Best model and parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Cross-Validation f1:\", grid_search.best_score_)\n\nFitting 5 folds for each of 243 candidates, totalling 1215 fits\nBest Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 250}\nBest Cross-Validation f1: 0.635791532822667\n\n\nMethod #2: RandomizedSearchCV\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\nparam_dist = {\n    'n_estimators': randint(50, 500),\n    'max_depth': randint(1, 20),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 20),\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\nrf = RandomForestClassifier()\nrand_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=20, cv=5,scoring='f1')\nrand_search.fit(X_train, y_train)\nprint('Best hyperparameters:', rand_search.best_params_)\n\nBest hyperparameters: {'max_depth': 14, 'max_features': 'log2', 'min_samples_leaf': 8, 'min_samples_split': 16, 'n_estimators': 280}\n\n\nSelect using judment based on the results from 2 methods\n\ndef randomforest(X, y,X1, y1):\n    \n    model = RandomForestClassifier(n_estimators=50,\n                                   max_depth=10,\n                                   random_state=42,\n                                   max_features='log2',\n                                   min_samples_leaf=5,\n                                   min_samples_split=5)\n\n    #model = RandomForestClassifier(random_state=42)\n    \n    # Train the model\n    model.fit(X, y)\n\n    # Predictions\n    y_pred = model.predict(X1)\n    # Probabilities\n    probabilities = model.predict_proba(X1)[:,1]\n\n    # Use Probabilites for ROC AUC\n    metric = roc_auc_score\n    metric_name = \"ROC AUC\"\n    auc_score = metric(y1, probabilities)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {auc_score:.4f}\")\n\n    # Use predictions for F1 Score.\n    metric = f1_score\n    metric_name = \"f1 score\"\n    precision_sc = metric(y1, y_pred)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {precision_sc:.4f}\")\n    \n\n    return model\n\n\n# Model Training\nrf_model = randomforest(X_train,y_train,X_test,y_test)\n\nRandomForestClassifier: ROC AUC on test dataset: 0.9751\nRandomForestClassifier: f1 score on test dataset: 0.6258\n\n\n\n\nModel Evaluation\n\n#CrossValidation Function: will use to compare results between train and test data used during cross-validation to asses over-fitting.\n\ndef cross_validation_check(model,X,y):\n\n    scoring = ['accuracy', 'precision','recall','f1', 'roc_auc']\n\n    cv_results = cross_validate(model, X, y, cv=5, # 3-fold to make it faster\n                          scoring=scoring,\n                          return_train_score=True) \n\n    print(f\"Cross validation results for Random Forest\")\n    for key in iter(cv_results):\n        mean = cv_results[key].mean()\n        print(f\"{key} Cross Validation results {mean:.2f}\")\n\n    return cv_results\n\n\ncv_results = cross_validation_check(rf_model,X,y0)\n\nCross validation results for Random Forest\nfit_time Cross Validation results 0.30\nscore_time Cross Validation results 0.02\ntest_accuracy Cross Validation results 0.90\ntrain_accuracy Cross Validation results 0.99\ntest_precision Cross Validation results 0.69\ntrain_precision Cross Validation results 0.97\ntest_recall Cross Validation results 0.46\ntrain_recall Cross Validation results 0.72\ntest_f1 Cross Validation results 0.45\ntrain_f1 Cross Validation results 0.83\ntest_roc_auc Cross Validation results 0.90\ntrain_roc_auc Cross Validation results 1.00\n\n\nModel seems to be overfitting F1 score delta between train and test is high\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = rf_model.predict(X_test)\n\nrm = confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(rm, annot=True, fmt='d')\nplt.xlabel('Predictred')\nplt.ylabel('Actual')\nplt.title('Contingency Table for Random Forest')\nplt.show()\n\n\n\n\n\n\n\n\nPrecision is very low, dataset is highly unbalanced, which is normal on manufacturing processes.\n\n# Plot the precision, recall and F1 for different thresholds to define prediction\n\n# Probabilities based on trained model\ny_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Evaluate metrics for thresholds from 0 to 1\nthresholds = np.linspace(0, 1, 100)\nprecisions, recalls, f1s = [], [], [] \n\n# check results for different thresholds.\nfor t in thresholds:\n    y_pred = (y_proba &gt;= t).astype(int)\n    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n    recalls.append(recall_score(y_test, y_pred))\n    f1s.append(f1_score(y_test, y_pred))\n\n# 6. Plot precision, recall, F1 vs threshold\nplt.figure(figsize=(8,5))\nplt.plot(thresholds, precisions, label='Precision', color='b')\nplt.plot(thresholds, recalls, label='Recall', color='g')\nplt.plot(thresholds, f1s, label='F1-score', color='r')\nplt.xlabel('Decision Threshold')\nplt.ylabel('Score')\nplt.title('Precision, Recall, and F1 vs Decision Threshold - Random Forest Model')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Creating new predictions based on optimal threshold based on previuos chart\n\ny_proba = rf_model.predict_proba(X_test)[:, 1]\nthreshold = 0.28 #based on optmil F1 score\nrf_preds_mod = (y_proba &gt;= threshold).astype(int)\n\nrm = confusion_matrix(y_test,rf_preds_mod)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(rm, annot=True, fmt='d')\nplt.xlabel('Predictred')\nplt.ylabel('Actual')\nplt.title('Contingency Table for Random Forest for 0.28 threshold')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Function to plot features of importance for Random Forest Model \n\ndef plot_feature_importances(importances, feature_names, model_type=\"Model\"):\n    \n    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n    feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='importance', y='feature', data=feature_importance_df)\n    plt.title(f'{model_type} - Feature Importances')\n    plt.xlabel('Importance Score')\n    plt.ylabel('Features')\n    plt.tight_layout()\n    plt.show()\n\n\nimportances_rfm = rf_model.feature_importances_    \nimportance_df_rfm = plot_feature_importances(importances_rfm, X_train.columns.to_list(), \"Random Forest Classifier\")\n\n\n\n\n\n\n\n\n\n\nHandling imbalanced target\n\n# Second Attemp using stratify sampling from train_test_split function\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y0, stratify=y0,  test_size=0.3, random_state=42)\n\n\nrf_model_2 = randomforest(X_train2,y_train2,X_test2,y_test2)\n\nRandomForestClassifier: ROC AUC on test dataset: 0.9751\nRandomForestClassifier: f1 score on test dataset: 0.6258\n\n\nSimilar results compare to first RF model\n\n# This attemp re-sampling\n# Code Source: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n# Synthetic Minority Oversampling Technique (SMOTE)\n\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\ndef randomforest_SMOTE(X, y,X1, y1):\n    \n    model = Pipeline([\n    ('smote', SMOTE(random_state=42)),\n    ('rf', RandomForestClassifier(n_estimators=50,  # Same hyperparameters as before\n                                   max_depth=10,\n                                   random_state=42,\n                                   max_features='log2',\n                                   min_samples_leaf=5,\n                                   min_samples_split=5))\n                    ])\n\n    # Train the model\n    model.fit(X, y)\n\n    y_pred = model.predict(X1)\n    probabilities = model.predict_proba(X1)[:,1]\n\n    metric = roc_auc_score\n    metric_name = \"ROC AUC\"\n    auc_score = metric(y1, probabilities)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {auc_score:.4f}\")\n\n    metric = f1_score\n    metric_name = \"f1 score\"\n    precision_sc = metric(y1, y_pred)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {precision_sc:.4f}\")\n    \n\n    return model\n\n\nrf_model_3 = randomforest_SMOTE(X_train2,y_train2,X_test2,y_test2)\nresults = cross_validation_check(rf_model_3,X,y0)\n\nRandomForestClassifier: ROC AUC on test dataset: 0.9743\nRandomForestClassifier: f1 score on test dataset: 0.5311\nCross validation results for Random Forest\nfit_time Cross Validation results 0.84\nscore_time Cross Validation results 0.02\ntest_accuracy Cross Validation results 0.87\ntrain_accuracy Cross Validation results 0.97\ntest_precision Cross Validation results 0.32\ntrain_precision Cross Validation results 0.55\ntest_recall Cross Validation results 0.71\ntrain_recall Cross Validation results 0.98\ntest_f1 Cross Validation results 0.40\ntrain_f1 Cross Validation results 0.70\ntest_roc_auc Cross Validation results 0.92\ntrain_roc_auc Cross Validation results 1.00\n\n\nWorse Results when adding SMOTE - synthetic oversampling\n\n# This attemp re-sampling\n# Using Under Sampling\n\n#Source: https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\nfrom imblearn.under_sampling import RandomUnderSampler\n\ndef randomforest_RuS(X, y,X1, y1):\n    \n    model = Pipeline([\n    ('rus', RandomUnderSampler(random_state=42,sampling_strategy=0.3)),\n    ('rf', RandomForestClassifier(random_state=42))\n                    ])\n\n    #model = RandomForestClassifier(random_state=42)\n    \n    # Train the model\n    model.fit(X, y)\n\n    y_pred = model.predict(X1)\n    probabilities = model.predict_proba(X1)[:,1]\n\n    metric = roc_auc_score\n    metric_name = \"ROC AUC\"\n    auc_score = metric(y1, probabilities)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {auc_score:.4f}\")\n\n    metric = f1_score\n    metric_name = \"f1 score\"\n    precision_sc = metric(y1, y_pred)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {precision_sc:.4f}\")\n    \n\n    return model\n\n\nrf_model_4 = randomforest_RuS(X_train2,y_train2,X_test2,y_test2)\nresults = cross_validation_check(rf_model_4,X,y0)\n\nRandomForestClassifier: ROC AUC on test dataset: 0.9646\nRandomForestClassifier: f1 score on test dataset: 0.5306\nCross validation results for Random Forest\nfit_time Cross Validation results 0.14\nscore_time Cross Validation results 0.02\ntest_accuracy Cross Validation results 0.87\ntrain_accuracy Cross Validation results 0.97\ntest_precision Cross Validation results 0.33\ntrain_precision Cross Validation results 0.50\ntest_recall Cross Validation results 0.76\ntrain_recall Cross Validation results 1.00\ntest_f1 Cross Validation results 0.42\ntrain_f1 Cross Validation results 0.67\ntest_roc_auc Cross Validation results 0.92\ntrain_roc_auc Cross Validation results 1.00\n\n\nNot significant improvement with undersampling method\n\n\nSecond Method - XGBOOST\nSource: https://xgboosting.com/xgboost-for-imbalanced-classification/\n\nimport xgboost as xgb\n\nscale_pos_weight = (len(y0) - np.sum(y0)) / np.sum(y0) #Intent is to give more weight to the minority class (1s on this case)\n\nxgmodel = xgb.XGBClassifier(\n    scale_pos_weight=scale_pos_weight,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=42\n)\n\nxgmodel.fit(X_train, y_train)\ny_pred = xgmodel.predict(X_test)\n\nf1 = f1_score(y_test, y_pred)\nprint(f\"F1 Score: {f1:.3f}\")\n\nF1 Score: 0.757\n\n\n\nresults = cross_validation_check(xgmodel,X_train,y_train)\n\nCross validation results for Random Forest\nfit_time Cross Validation results 0.12\nscore_time Cross Validation results 0.01\ntest_accuracy Cross Validation results 0.98\ntrain_accuracy Cross Validation results 1.00\ntest_precision Cross Validation results 0.74\ntrain_precision Cross Validation results 1.00\ntest_recall Cross Validation results 0.72\ntrain_recall Cross Validation results 1.00\ntest_f1 Cross Validation results 0.73\ntrain_f1 Cross Validation results 1.00\ntest_roc_auc Cross Validation results 0.97\ntrain_roc_auc Cross Validation results 1.00\n\n\nBetter results than Random Forest. Still there is some over-fitting on the model but less than RF\n\n# Contingency Table\ny_pred = xgmodel.predict(X_test)\n\nrm = confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(rm, annot=True, fmt='d')\nplt.xlabel('Predictred')\nplt.ylabel('Actual')\nplt.title('Contingency Table for XGBoost with Sample Weighting')\nplt.show()\n\n\n\n\n\n\n\n\nTesting if can improve model with hyperparameter tuning\n\n# Same method as with Random Forest\nparam_dist = {\n    'max_depth': randint(3, 10),\n    'learning_rate': uniform(0.01, 0.3),\n    'n_estimators': randint(50, 300),\n    'subsample': uniform(0.5, 0.5),\n    'colsample_bytree': uniform(0.5, 0.5)\n}\n\nrand_search = RandomizedSearchCV(xgmodel, param_distributions=param_dist, n_iter=20, cv=5,scoring='f1')\nrand_search.fit(X_train, y_train)\nprint('Best hyperparameters:', rand_search.best_params_)\n\nBest hyperparameters: {'colsample_bytree': np.float64(0.8426068608991865), 'learning_rate': np.float64(0.2589806507914076), 'max_depth': 5, 'n_estimators': 216, 'subsample': np.float64(0.9010342428678821)}\n\n\n\nxgmodel2 = xgb.XGBClassifier(\n    scale_pos_weight=scale_pos_weight,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=42,\n    colsample_bytree=0.935,\n    learning_rate=0.134,\n    max_depth=8,\n    n_estimators=240,\n    subsample=0.8\n)\n\nxgmodel2.fit(X_train, y_train)\ny_pred = xgmodel.predict(X_test)\n\nf1 = f1_score(y_test, y_pred)\nprint(f\"F1 Score: {f1:.3f}\")\n\nF1 Score: 0.757\n\n\nInital XGBoost model performed better\n\nresults = cross_validation_check(xgmodel2,X_train,y_train)\n\nCross validation results for Random Forest\nfit_time Cross Validation results 0.25\nscore_time Cross Validation results 0.02\ntest_accuracy Cross Validation results 0.98\ntrain_accuracy Cross Validation results 1.00\ntest_precision Cross Validation results 0.76\ntrain_precision Cross Validation results 1.00\ntest_recall Cross Validation results 0.71\ntrain_recall Cross Validation results 1.00\ntest_f1 Cross Validation results 0.73\ntrain_f1 Cross Validation results 1.00\ntest_roc_auc Cross Validation results 0.97\ntrain_roc_auc Cross Validation results 1.00\n\n\n\n# Plot the precision, recall and F1 for different thresholds to define prediction\n\n# Probabilities based on trained model\ny_proba = xgmodel.predict_proba(X_test)[:, 1]\n\n# Evaluate metrics for thresholds from 0 to 1\nthresholds = np.linspace(0, 1, 100)\nprecisions, recalls, f1s = [], [], [] \n\n# check results for different thresholds.\nfor t in thresholds:\n    y_pred = (y_proba &gt;= t).astype(int)\n    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n    recalls.append(recall_score(y_test, y_pred))\n    f1s.append(f1_score(y_test, y_pred))\n\n# 6. Plot precision, recall, F1 vs threshold\nplt.figure(figsize=(8,5))\nplt.plot(thresholds, precisions, label='Precision', color='b')\nplt.plot(thresholds, recalls, label='Recall', color='g')\nplt.plot(thresholds, f1s, label='F1-score', color='r')\nplt.xlabel('Decision Threshold')\nplt.ylabel('Score')\nplt.title('Precision, Recall, and F1 vs Decision Threshold - XGBoost model')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom xgboost import plot_importance\n\nplot_importance(xgmodel, importance_type='weight')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFAILURE TYPE PREDICTION\n\nX_trainF, X_testF, y_trainF, y_testF = train_test_split(X, y1,  test_size=0.3, random_state=42)\n\n\ndef randomforest_m(X, y,X1, y1):\n    \n    model = RandomForestClassifier(n_estimators=50,\n                                   max_depth=10,\n                                   random_state=42,\n                                   max_features='log2',\n                                   min_samples_leaf=5,\n                                   min_samples_split=5)\n\n    #model = RandomForestClassifier(random_state=42)\n    \n    # Train the model\n    model.fit(X, y)\n\n    y_pred = model.predict(X1)\n    probabilities = model.predict_proba(X1)\n\n    metric_name = \"ROC AUC\"\n    auc_score = roc_auc_score(y1, probabilities, multi_class='ovr')\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {auc_score:.4f}\")\n\n    f1_macro = f1_score(y1, y_pred, average='macro')\n    f1_micro = f1_score(y1, y_pred, average='micro')\n    print(f\"RandomForestClassifier: F1 Macro on test dataset: {f1_macro:.4f}\")\n    print(f\"RandomForestClassifier: F1 Micro on test dataset: {f1_micro:.4f}\")\n    \n    f1_per_class = f1_score(y1, y_pred, average=None)\n\n    print(\"F1 score per class:\", f1_per_class)\n\n    return model\n\n\nrfm_model = randomforest_m(X_trainF,y_trainF,X_testF,y_testF)\n\nRandomForestClassifier: ROC AUC on test dataset: 0.9009\nRandomForestClassifier: F1 Macro on test dataset: 0.4965\nRandomForestClassifier: F1 Micro on test dataset: 0.9800\nF1 score per class: [0.68421053 0.9897506  0.53333333 0.77192982 0.         0.        ]\n\n\n\ny_pred_m = rfm_model.predict(X_testF)\nrm = confusion_matrix(y_testF,y_pred_m)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(rm, annot=True, fmt='d')\nplt.xlabel('Predictred')\nplt.ylabel('Actual')\nplt.title('Contingency Table for Random Forest MultiClass')\nplt.show()\n\n\n\n\n\n\n\n\nClass Names: ‘Heat Dissipation Failure’: 0, ‘No Failure’: 1, ‘Overstrain Failure’: 2, ‘Power Failure’: 3, ‘Random Failures’: 4, ‘Tool Wear Failure’: 5\n\nclasses, counts = np.unique(y_train, return_counts=True)\ninv_freq = 1.0 / counts\nclass_weights = dict(zip(classes, inv_freq))\nsample_weights = np.array([class_weights[label] for label in y_train])\n\nxgmodelm = xgb.XGBClassifier(\n    sample_weight=sample_weights,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=42\n)\n\nxgmodelm.fit(X_trainF, y_trainF)\ny_pred = xgmodelm.predict(X_testF)\n\nf1_per_class = f1_score(y_testF, y_pred, average=None)\n\nprint(\"F1 score per class:\", f1_per_class)\n\nF1 score per class: [0.95652174 0.9936546  0.76190476 0.8        0.         0.        ]\n\n\n\nrm = confusion_matrix(y_testF,y_pred)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(rm, annot=True, fmt='d')\nplt.xlabel('Predictred')\nplt.ylabel('Actual')\nplt.title('Contingency Table for XG Boost MultiClass')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom xgboost import plot_importance\n\nplot_importance(xgmodelm, importance_type='weight')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExplore Clustering as new Feature to improve performance\n\n# Will use only numeric columns for clustering\nnum_cols = X.select_dtypes(include=\"number\").columns.to_list()\nX_cl = X[num_cols]\n\n\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.cluster import KMeans\nfrom matplotlib.ticker import MultipleLocator\n\n\n#source: https://towardsdatascience.com/calinski-harabasz-index-for-k-means-clustering-evaluation-using-python-4fefeeb2988e/\nresults = {}\n\nfor i in range(2,20):\n    kmeans = KMeans(n_clusters=i, random_state=42)\n    labels = kmeans.fit_predict(X_cl)\n    db_index = calinski_harabasz_score(X_cl, labels)\n    results.update({i: db_index})\n\nfig, ax = plt.subplots()\n\nax.plot(list(results.keys()), list(results.values()))\nax.xaxis.set_major_locator(MultipleLocator(1))\nax.grid(True)\nax.set_title('Calinski-Harabasz method to determine the optimal number of clusters')\nax.set_xlabel(\"Number of clusters\")\nax.set_ylabel(\"Calinski-Harabasz Index\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels = kmeans.fit_predict(X_cl)\nkmeans_labels = labels\n\n\nplt_df = X_cl.copy()\nplt_df['cluster'] = kmeans_labels\nplt.figure(figsize=(10,6))\nsns.pairplot(plt_df, diag_kind='kde', hue='cluster')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# Create new feature calculating the distance to the center of the cluster\nall_distances = kmeans.transform(X_cl)\nassigned_distances = all_distances[np.arange(len(X_cl)), labels]\nX['dist_cluster_center'] = assigned_distances\nX.head()\n\n\n\n\n\n\n\n\nAir_temperature_K\nProcess_temperature_K\nRotational_speed_rpm\nTorque_Nm\nTool_wear_min\nType_H\nType_L\nType_M\ndist_cluster_center\n\n\n\n\n0\n-0.952389\n-0.947360\n0.068185\n0.282200\n-1.695984\nFalse\nFalse\nTrue\n1.707273\n\n\n1\n-0.902393\n-0.879959\n-0.729472\n0.633308\n-1.648852\nFalse\nTrue\nFalse\n1.671828\n\n\n2\n-0.952389\n-1.014761\n-0.227450\n0.944290\n-1.617430\nFalse\nTrue\nFalse\n1.700837\n\n\n3\n-0.902393\n-0.947360\n-0.590021\n-0.048845\n-1.586009\nFalse\nTrue\nFalse\n1.602883\n\n\n4\n-0.902393\n-0.879959\n-0.729472\n0.001313\n-1.554588\nFalse\nTrue\nFalse\n1.585680\n\n\n\n\n\n\n\n\nX_traink, X_testk, y_traink, y_testk = train_test_split(X, y0, stratify=y0,  test_size=0.3, random_state=42)\n\n\nscale_pos_weight = (len(y0) - np.sum(y0)) / np.sum(y0) #Intent is to give more weight to the minority class (1s on this case)\n\nxgmodel_k = xgb.XGBClassifier(\n    scale_pos_weight=scale_pos_weight,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=42\n)\n\nxgmodel_k.fit(X_traink, y_traink)\ny_pred_k = xgmodel_k.predict(X_testk)\n\nf1 = f1_score(y_testk, y_pred_k)\nprint(f\"F1 Score: {f1:.3f}\")\n\nF1 Score: 0.745"
  }
]