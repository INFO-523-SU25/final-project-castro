[
  {
    "objectID": "src/Model_Training_PDM.html",
    "href": "src/Model_Training_PDM.html",
    "title": "Model Training Defect Detection",
    "section": "",
    "text": "Author: Cesar Castro M\n\n\nDate: 08/11/2025\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, f1_score, precision_score, recall_score\n\nfrom sklearn.model_selection import cross_validate\nimport warnings\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\n\nmfg_df = pd.read_csv('..\\data\\processed\\predictive_maintenance_ready.csv')\n\n\nmfg_df.columns = mfg_df.columns.str.replace('[\\[\\]&lt;&gt;]', '', regex=True).astype(str)\nmfg_df.columns = mfg_df.columns.str.replace(' ', '_').astype(str)\n\nmfg_df.head()\n\n\n\n\n\n\n\n\nAir_temperature_K\nProcess_temperature_K\nRotational_speed_rpm\nTorque_Nm\nTool_wear_min\nTarget\nFailureType_encoded\nType_H\nType_L\nType_M\n\n\n\n\n0\n-0.952389\n-0.947360\n0.068185\n0.282200\n-1.695984\n0\n1\nFalse\nFalse\nTrue\n\n\n1\n-0.902393\n-0.879959\n-0.729472\n0.633308\n-1.648852\n0\n1\nFalse\nTrue\nFalse\n\n\n2\n-0.952389\n-1.014761\n-0.227450\n0.944290\n-1.617430\n0\n1\nFalse\nTrue\nFalse\n\n\n3\n-0.902393\n-0.947360\n-0.590021\n-0.048845\n-1.586009\n0\n1\nFalse\nTrue\nFalse\n\n\n4\n-0.902393\n-0.879959\n-0.729472\n0.001313\n-1.554588\n0\n1\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\nX =mfg_df.drop(columns=['Target','FailureType_encoded'],axis=1)\ny0 = mfg_df['Target']\ny1 = mfg_df['FailureType_encoded']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y0, stratify=y0,  test_size=0.3, random_state=42)\n\n\n#Data is highly unbalanced\nprint(f'Total fails: {y0.sum()} out of total data points {len(y0)}')\n\nTotal fails: 339 out of total data points 10000\n\n\n\n# Will Try Multiple Models based on some research\n# Rerference: https://github.com/dataprofessor/code/blob/master/python/comparing-classifiers.ipynb\n\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nnames = [\"Nearest_Neighbors\",  \"Gradient_Boosting\", \"Decision_Tree\",\n        \"Extra_Trees\", \"Random_Forest\", \"Neural_Net\", \"AdaBoost\",\n        \"Naive_Bayes\", \"QDA\",'LogisticRegression']\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0),\n    DecisionTreeClassifier(max_depth=5),\n    ExtraTreesClassifier(n_estimators=10, min_samples_split=2),\n    RandomForestClassifier(random_state=42),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(n_estimators=100),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(random_state=42)\n    ]\n\n\nscores = []\nmetric = roc_auc_score\nfor name, clf in zip(names, classifiers):\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    y_pred = clf.predict(X_test)\n    y_prob = clf.predict_proba(X_test)[:,1]\n    rscore = metric(y_test, y_prob)\n    scores.append(rscore)\n\n#https://github.com/dataprofessor/code/blob/master/python/comparing-classifiers.ipynb\ndf = pd.DataFrame()\ndf['name'] = names\ndf['ROC_AUC'] = scores\ndf\n\n\n\n\n\n\n\n\nname\nROC_AUC\n\n\n\n\n0\nNearest_Neighbors\n0.784725\n\n\n1\nGradient_Boosting\n0.818907\n\n\n2\nDecision_Tree\n0.932831\n\n\n3\nExtra_Trees\n0.895799\n\n\n4\nRandom_Forest\n0.972481\n\n\n5\nNeural_Net\n0.917079\n\n\n6\nAdaBoost\n0.899753\n\n\n7\nNaive_Bayes\n0.827014\n\n\n8\nQDA\n0.838547\n\n\n9\nLogisticRegression\n0.880628\n\n\n\n\n\n\n\n\nscores = []\nmetric = f1_score\nfor name, clf in zip(names, classifiers):\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    y_pred = clf.predict(X_test)\n    y_prob = clf.predict_proba(X_test)[:,1]\n    rscore = metric(y_test, y_pred)\n    scores.append(rscore)\n\n#https://github.com/dataprofessor/code/blob/master/python/comparing-classifiers.ipynb\ndf = pd.DataFrame()\ndf['name'] = names\ndf['f1 score'] = scores\ndf\n\n\n\n\n\n\n\n\nname\nf1 score\n\n\n\n\n0\nNearest_Neighbors\n0.423077\n\n\n1\nGradient_Boosting\n0.513966\n\n\n2\nDecision_Tree\n0.503145\n\n\n3\nExtra_Trees\n0.381679\n\n\n4\nRandom_Forest\n0.567742\n\n\n5\nNeural_Net\n0.234375\n\n\n6\nAdaBoost\n0.473373\n\n\n7\nNaive_Bayes\n0.198758\n\n\n8\nQDA\n0.278788\n\n\n9\nLogisticRegression\n0.224000\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the base model\nrf = RandomForestClassifier(random_state=42)\n\n# Randomly Choosing Parameters for testing\nparam_grid = {\n    'n_estimators': [50, 250, 500],\n    'max_depth': [5, 10, 20],\n    'min_samples_split': [5, 15, 30],\n    'min_samples_leaf': [5, 10, 30],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\n\ngrid_search = GridSearchCV(\n    estimator=rf,\n    param_grid=param_grid,\n    cv=5,                   # 5-fold cross-validation\n    scoring='f1',     \n    n_jobs=-1,              # Use all CPU cores\n    verbose=1\n)\n\n# Fit to training data\ngrid_search.fit(X_train, y_train)\n\n# Best model and parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Cross-Validation f1:\", grid_search.best_score_)\n\nFitting 5 folds for each of 243 candidates, totalling 1215 fits\nBest Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 250}\nBest Cross-Validation f1: 0.635791532822667\n\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\nparam_dist = {\n    'n_estimators': randint(50, 500),\n    'max_depth': randint(1, 20),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 20),\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\nrf = RandomForestClassifier()\nrand_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=20, cv=5,scoring='f1')\nrand_search.fit(X_train, y_train)\nprint('Best hyperparameters:', rand_search.best_params_)\n\nBest hyperparameters: {'max_depth': 8, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 444}\n\n\n\ndef randomforest(X, y,X1, y1):\n    \n    model = RandomForestClassifier(n_estimators=50,\n                                   max_depth=10,\n                                   random_state=42,\n                                   max_features='log2',\n                                   min_samples_leaf=5,\n                                   min_samples_split=5)\n\n    #model = RandomForestClassifier(random_state=42)\n    \n    # Train the model\n    model.fit(X, y)\n\n    y_pred = model.predict(X1)\n    probabilities = model.predict_proba(X1)[:,1]\n\n    metric = roc_auc_score\n    metric_name = \"ROC AUC\"\n    auc_score = metric(y1, probabilities)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {auc_score:.4f}\")\n\n    metric = f1_score\n    metric_name = \"f1 score\"\n    precision_sc = metric(y1, y_pred)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {precision_sc:.4f}\")\n    \n\n    return model\n\n\nrf_model = randomforest(X_train,y_train,X_test,y_test)\n\nRandomForestClassifier: ROC AUC on test dataset: 0.9751\nRandomForestClassifier: f1 score on test dataset: 0.6258\n\n\n\n#CrossValidation\n\ndef cross_validation_check(model,X,y):\n\n    scoring = ['accuracy', 'precision','recall','f1', 'roc_auc']\n\n    cv_results = cross_validate(model, X, y, cv=5, # 3-fold to make it faster\n                          scoring=scoring,\n                          return_train_score=True) \n\n    print(f\"Cross validation results for Random Forest\")\n    for key in iter(cv_results):\n        mean = cv_results[key].mean()\n        print(f\"{key} Cross Validation results {mean:.2f}\")\n\n    return cv_results\n\n\ncv_results = cross_validation_check(rf_model,X,y0)\n\nCross validation results for Random Forest\nfit_time Cross Validation results 0.24\nscore_time Cross Validation results 0.01\ntest_accuracy Cross Validation results 0.90\ntrain_accuracy Cross Validation results 0.99\ntest_precision Cross Validation results 0.69\ntrain_precision Cross Validation results 0.97\ntest_recall Cross Validation results 0.46\ntrain_recall Cross Validation results 0.72\ntest_f1 Cross Validation results 0.45\ntrain_f1 Cross Validation results 0.83\ntest_roc_auc Cross Validation results 0.90\ntrain_roc_auc Cross Validation results 1.00\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = rf_model.predict(X_test)\n\nrm = confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(rm, annot=True, fmt='d')\nplt.xlabel('Predictred')\nplt.ylabel('Actual')\nplt.title('Contingency Table for Random Forest')\nplt.show()\n\n\n\n\n\n\n\n\nPrecision is very low, dataset is highly unbalanced, which is normal on manufacturing processes.\n\n\n# Get prediction probabilities for the positive class\ny_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Evaluate metrics for thresholds from 0 to 1\nthresholds = np.linspace(0, 1, 100)\nprecisions, recalls, f1s = [], [], []\n\nfor t in thresholds:\n    y_pred = (y_proba &gt;= t).astype(int)\n    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n    recalls.append(recall_score(y_test, y_pred))\n    f1s.append(f1_score(y_test, y_pred))\n\n# 6. Plot precision, recall, F1 vs threshold\nplt.figure(figsize=(8,5))\nplt.plot(thresholds, precisions, label='Precision', color='b')\nplt.plot(thresholds, recalls, label='Recall', color='g')\nplt.plot(thresholds, f1s, label='F1-score', color='r')\nplt.xlabel('Decision Threshold')\nplt.ylabel('Score')\nplt.title('Precision, Recall, and F1 vs Decision Threshold')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ny_proba = rf_model.predict_proba(X_test)[:, 1]\nthreshold = 0.28\nrf_preds_mod = (y_proba &gt;= threshold).astype(int)\n\nrm = confusion_matrix(y_test,rf_preds_mod)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(rm, annot=True, fmt='d')\nplt.xlabel('Predictred')\nplt.ylabel('Actual')\nplt.title('Contingency Table for Random Forest for 0.28 threshold')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Second Attemp using stratify sampling from train_test_split function\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y0, stratify=y0,  test_size=0.3, random_state=42)\n\n\nrf_model_2 = randomforest(X_train2,y_train2,X_test2,y_test2)\n\nRandomForestClassifier: ROC AUC on test dataset: 0.9751\nRandomForestClassifier: f1 score on test dataset: 0.6258\n\n\n\n# This attemp re-sampling\n# Code Source: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n# Synthetic Minority Oversampling Technique (SMOTE)\n\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\ndef randomforest_SMOTE(X, y,X1, y1):\n    \n    model = Pipeline([\n    ('smote', SMOTE(random_state=42)),\n    ('rf', RandomForestClassifier(n_estimators=50,\n                                   max_depth=10,\n                                   random_state=42,\n                                   max_features='log2',\n                                   min_samples_leaf=5,\n                                   min_samples_split=5))\n                    ])\n\n    #model = RandomForestClassifier(random_state=42)\n    \n    # Train the model\n    model.fit(X, y)\n\n    y_pred = model.predict(X1)\n    probabilities = model.predict_proba(X1)[:,1]\n\n    metric = roc_auc_score\n    metric_name = \"ROC AUC\"\n    auc_score = metric(y1, probabilities)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {auc_score:.4f}\")\n\n    metric = f1_score\n    metric_name = \"f1 score\"\n    precision_sc = metric(y1, y_pred)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {precision_sc:.4f}\")\n    \n\n    return model\n\n\nrf_model_3 = randomforest_SMOTE(X_train2,y_train2,X_test2,y_test2)\nresults = cross_validation_check(rf_model_3,X,y0)\n\nRandomForestClassifier: ROC AUC on test dataset: 0.9743\nRandomForestClassifier: f1 score on test dataset: 0.5311\nCross validation results for Random Forest\nfit_time Cross Validation results 0.63\nscore_time Cross Validation results 0.01\ntest_accuracy Cross Validation results 0.87\ntrain_accuracy Cross Validation results 0.97\ntest_precision Cross Validation results 0.32\ntrain_precision Cross Validation results 0.55\ntest_recall Cross Validation results 0.71\ntrain_recall Cross Validation results 0.98\ntest_f1 Cross Validation results 0.40\ntrain_f1 Cross Validation results 0.70\ntest_roc_auc Cross Validation results 0.92\ntrain_roc_auc Cross Validation results 1.00\n\n\n\n# This attemp re-sampling\n# Using Under Sampling\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\ndef randomforest_RuS(X, y,X1, y1):\n    \n    model = Pipeline([\n    ('rus', RandomUnderSampler(random_state=42,sampling_strategy=0.3)),\n    ('rf', RandomForestClassifier(random_state=42))\n                    ])\n\n    #model = RandomForestClassifier(random_state=42)\n    \n    # Train the model\n    model.fit(X, y)\n\n    y_pred = model.predict(X1)\n    probabilities = model.predict_proba(X1)[:,1]\n\n    metric = roc_auc_score\n    metric_name = \"ROC AUC\"\n    auc_score = metric(y1, probabilities)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {auc_score:.4f}\")\n\n    metric = f1_score\n    metric_name = \"f1 score\"\n    precision_sc = metric(y1, y_pred)\n    print(f\"RandomForestClassifier: {metric_name} on test dataset: {precision_sc:.4f}\")\n    \n\n    return model\n\n\nrf_model_4 = randomforest_RuS(X_train2,y_train2,X_test2,y_test2)\nresults = cross_validation_check(rf_model_4,X,y0)\n\nRandomForestClassifier: ROC AUC on test dataset: 0.9646\nRandomForestClassifier: f1 score on test dataset: 0.5306\nCross validation results for Random Forest\nfit_time Cross Validation results 0.10\nscore_time Cross Validation results 0.02\ntest_accuracy Cross Validation results 0.87\ntrain_accuracy Cross Validation results 0.97\ntest_precision Cross Validation results 0.33\ntrain_precision Cross Validation results 0.50\ntest_recall Cross Validation results 0.76\ntrain_recall Cross Validation results 1.00\ntest_f1 Cross Validation results 0.42\ntrain_f1 Cross Validation results 0.67\ntest_roc_auc Cross Validation results 0.92\ntrain_roc_auc Cross Validation results 1.00\n\n\n\n\n# Get prediction probabilities for the positive class\ny_proba = rf_model_4.predict_proba(X_test)[:, 1]\n\n# Evaluate metrics for thresholds from 0 to 1\nthresholds = np.linspace(0, 1, 100)\nprecisions, recalls, f1s = [], [], []\n\nfor t in thresholds:\n    y_pred = (y_proba &gt;= t).astype(int)\n    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n    recalls.append(recall_score(y_test, y_pred))\n    f1s.append(f1_score(y_test, y_pred))\n\n# 6. Plot precision, recall, F1 vs threshold\nplt.figure(figsize=(8,5))\nplt.plot(thresholds, precisions, label='Precision', color='b')\nplt.plot(thresholds, recalls, label='Recall', color='g')\nplt.plot(thresholds, f1s, label='F1-score', color='r')\nplt.xlabel('Decision Threshold')\nplt.ylabel('Score')\nplt.title('Precision, Recall, and F1 vs Decision Threshold')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport xgboost as xgb\n\nscale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n\nxgmodel = xgb.XGBClassifier(\n    scale_pos_weight=scale_pos_weight,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=42\n)\n\nxgmodel.fit(X_train, y_train)\ny_pred = xgmodel.predict(X_test)\n\nf1 = f1_score(y_test, y_pred)\nprint(f\"F1 Score: {f1:.3f}\")\n\nF1 Score: 0.727\n\n\n\nresults = cross_validation_check(xgmodel,X_train,y_train)\n\nCross validation results for Random Forest\nfit_time Cross Validation results 0.08\nscore_time Cross Validation results 0.02\ntest_accuracy Cross Validation results 0.98\ntrain_accuracy Cross Validation results 1.00\ntest_precision Cross Validation results 0.71\ntrain_precision Cross Validation results 1.00\ntest_recall Cross Validation results 0.71\ntrain_recall Cross Validation results 1.00\ntest_f1 Cross Validation results 0.71\ntrain_f1 Cross Validation results 1.00\ntest_roc_auc Cross Validation results 0.97\ntrain_roc_auc Cross Validation results 1.00\n\n\n\n\ny_pred = xgmodel.predict(X_test)\n\nrm = confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(rm, annot=True, fmt='d')\nplt.xlabel('Predictred')\nplt.ylabel('Actual')\nplt.title('Contingency Table for Random Forest')\nplt.show()\n\n\n\n\n\n\n\n\n\nparam_dist = {\n    'max_depth': randint(3, 10),\n    'learning_rate': uniform(0.01, 0.3),\n    'n_estimators': randint(50, 300),\n    'subsample': uniform(0.5, 0.5),\n    'colsample_bytree': uniform(0.5, 0.5),\n    'scale_pos_weight': [1, (y_train == 0).sum() / (y_train == 1).sum()]\n}\n\nrand_search = RandomizedSearchCV(xgmodel, param_distributions=param_dist, n_iter=20, cv=5,scoring='f1')\nrand_search.fit(X_train, y_train)\nprint('Best hyperparameters:', rand_search.best_params_)\n\nBest hyperparameters: {'colsample_bytree': np.float64(0.884809572237244), 'learning_rate': np.float64(0.08881798751800665), 'max_depth': 6, 'n_estimators': 290, 'scale_pos_weight': np.float64(28.535864978902953), 'subsample': np.float64(0.8533428495380565)}\n\n\n\nxgmodel = xgb.XGBClassifier(\n    scale_pos_weight=scale_pos_weight,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=42,\n    colsample_bytree=0.70,\n    learning_rate=0.096,\n    max_depth=8,\n    n_estimators=150,\n    subsample=0.78\n)\n\nxgmodel.fit(X_train, y_train)\ny_pred = xgmodel.predict(X_test)\n\nf1 = f1_score(y_test, y_pred)\nprint(f\"F1 Score: {f1:.3f}\")\n\nF1 Score: 0.700\n\n\n\nresults = cross_validation_check(xgmodel,X_train,y_train)\n\nCross validation results for Random Forest\nfit_time Cross Validation results 0.17\nscore_time Cross Validation results 0.01\ntest_accuracy Cross Validation results 0.98\ntrain_accuracy Cross Validation results 1.00\ntest_precision Cross Validation results 0.71\ntrain_precision Cross Validation results 0.99\ntest_recall Cross Validation results 0.70\ntrain_recall Cross Validation results 1.00\ntest_f1 Cross Validation results 0.70\ntrain_f1 Cross Validation results 1.00\ntest_roc_auc Cross Validation results 0.97\ntrain_roc_auc Cross Validation results 1.00"
  },
  {
    "objectID": "src/Data_Exploration_MFG6G.html",
    "href": "src/Data_Exploration_MFG6G.html",
    "title": "Data Exploration/Cleaning for Manufacturing 6G dataset",
    "section": "",
    "text": "Author: Cesar Castro M\n\n\nDate: 08/11/2025\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\n\n\nmfg_data = pd.read_csv('..\\data\\manufacturing_6G_dataset.csv')\n\n\nmfg_data.head()\n\n\n\n\n\n\n\n\nTimestamp\nMachine_ID\nOperation_Mode\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_%\nQuality_Control_Defect_Rate_%\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_%\nEfficiency_Status\n\n\n\n\n0\n2024-01-01 00:00:00\n39\nIdle\n74.137590\n3.500595\n8.612162\n10.650542\n0.207764\n7.751261\n477.657391\n0.344650\n14.965470\nLow\n\n\n1\n2024-01-01 00:01:00\n29\nActive\n84.264558\n3.355928\n2.268559\n29.111810\n2.228464\n4.989172\n398.174747\n0.769848\n7.678270\nLow\n\n\n2\n2024-01-01 00:02:00\n15\nActive\n44.280102\n2.079766\n6.144105\n18.357292\n1.639416\n0.456816\n108.074959\n0.987086\n8.198391\nLow\n\n\n3\n2024-01-01 00:03:00\n43\nActive\n40.568502\n0.298238\n4.067825\n29.153629\n1.161021\n4.582974\n329.579410\n0.983390\n2.740847\nMedium\n\n\n4\n2024-01-01 00:04:00\n8\nIdle\n75.063817\n0.345810\n6.225737\n34.029191\n4.796520\n2.287716\n159.113525\n0.573117\n12.100686\nLow\n\n\n\n\n\n\n\n\n# Setting right feature type\nmfg_data['Timestamp'] = pd.to_datetime(mfg_data['Timestamp'])\nmfg_data['Machine_ID'] = mfg_data['Machine_ID'].astype('object')\n\n\n#Find Categorical Features on childcare_costs dataset\ndef cat_features_explore(df):\n    def calc_unique_counts(df,col_name):\n        col = df[col_name]\n        unique_val = col.unique()\n        plt.figure(figsize=(12,4))\n        sns.countplot(data=df, x=col_name)\n        plt.title(f'Categories for {col_name}')\n        plt.tight_layout()\n        plt.show()\n        return len(unique_val)\n\n    cat_cols = mfg_data.select_dtypes(include=\"object\").columns.to_list()\n    count_dic = {col:calc_unique_counts(mfg_data,col) for col in cat_cols}\n    print(count_dic)    \n \ncat_features_explore(mfg_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'Machine_ID': 50, 'Operation_Mode': 3, 'Efficiency_Status': 3}\n\n\n\ndef plot_hist_qq(df,col_name):\n    col = df[col_name]\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2,nrows=1,figsize=(8, 4))\n    sns.histplot(col, linewidth=1,ax=ax1)\n    #sns.kdeplot(col, linewidth=5,ax=ax1)\n    ax1.set_title(col_name + ' Histogram')\n\n    sm.qqplot(col,line='s',ax=ax2)\n    ax2.set_title(col_name + ' Q-Q Plot')\n    plt.tight_layout()\n    plt.show()\n\ndef calculate_stats(df,col_name):\n    print (f\"{col_name} Mean: {df[col_name].mean():.2f}\")\n    print (f\"{col_name} Median: {df[col_name].median():.2f}\")\n    print (f\"{col_name} Max: {df[col_name].max():.2f}\")\n    print (f\"{col_name} Min: {df[col_name].min():.2f}\")\n    print (f\"{col_name} Standard Deviation: {df[col_name].std():.2f}\")    \n    print (f\"{col_name} Number of Points: {len(df[col_name]):.0f}\")    \n\nfor col in mfg_data.select_dtypes(include=\"number\").columns.to_list():\n    plot_hist_qq(mfg_data[mfg_data['Machine_ID']==2],col) \n    calculate_stats(mfg_data[mfg_data['Machine_ID']==2],col)\n\n\n\n\n\n\n\n\nTemperature_C Mean: 59.81\nTemperature_C Median: 59.98\nTemperature_C Max: 89.94\nTemperature_C Min: 30.03\nTemperature_C Standard Deviation: 17.64\nTemperature_C Number of Points: 2041\n\n\n\n\n\n\n\n\n\nVibration_Hz Mean: 2.53\nVibration_Hz Median: 2.59\nVibration_Hz Max: 5.00\nVibration_Hz Min: 0.10\nVibration_Hz Standard Deviation: 1.42\nVibration_Hz Number of Points: 2041\n\n\n\n\n\n\n\n\n\nPower_Consumption_kW Mean: 5.72\nPower_Consumption_kW Median: 5.68\nPower_Consumption_kW Max: 10.00\nPower_Consumption_kW Min: 1.50\nPower_Consumption_kW Standard Deviation: 2.46\nPower_Consumption_kW Number of Points: 2041\n\n\n\n\n\n\n\n\n\nNetwork_Latency_ms Mean: 25.45\nNetwork_Latency_ms Median: 25.34\nNetwork_Latency_ms Max: 49.98\nNetwork_Latency_ms Min: 1.02\nNetwork_Latency_ms Standard Deviation: 14.02\nNetwork_Latency_ms Number of Points: 2041\n\n\n\n\n\n\n\n\n\nPacket_Loss_% Mean: 2.52\nPacket_Loss_% Median: 2.56\nPacket_Loss_% Max: 5.00\nPacket_Loss_% Min: 0.01\nPacket_Loss_% Standard Deviation: 1.41\nPacket_Loss_% Number of Points: 2041\n\n\n\n\n\n\n\n\n\nQuality_Control_Defect_Rate_% Mean: 5.01\nQuality_Control_Defect_Rate_% Median: 5.00\nQuality_Control_Defect_Rate_% Max: 9.98\nQuality_Control_Defect_Rate_% Min: 0.00\nQuality_Control_Defect_Rate_% Standard Deviation: 2.90\nQuality_Control_Defect_Rate_% Number of Points: 2041\n\n\n\n\n\n\n\n\n\nProduction_Speed_units_per_hr Mean: 274.51\nProduction_Speed_units_per_hr Median: 270.07\nProduction_Speed_units_per_hr Max: 499.98\nProduction_Speed_units_per_hr Min: 50.35\nProduction_Speed_units_per_hr Standard Deviation: 129.23\nProduction_Speed_units_per_hr Number of Points: 2041\n\n\n\n\n\n\n\n\n\nPredictive_Maintenance_Score Mean: 0.49\nPredictive_Maintenance_Score Median: 0.48\nPredictive_Maintenance_Score Max: 1.00\nPredictive_Maintenance_Score Min: 0.00\nPredictive_Maintenance_Score Standard Deviation: 0.29\nPredictive_Maintenance_Score Number of Points: 2041\n\n\n\n\n\n\n\n\n\nError_Rate_% Mean: 7.52\nError_Rate_% Median: 7.48\nError_Rate_% Max: 15.00\nError_Rate_% Min: 0.00\nError_Rate_% Standard Deviation: 4.38\nError_Rate_% Number of Points: 2041\n\n\n\nmfg_data.select_dtypes(include=\"number\").columns.to_list()\nplt.figure(figsize=(10,6))\nsns.pairplot(mfg_data, diag_kind='kde',hue='Efficiency_Status')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nDATA PREPARATION FOR ANOMALY DETECTION\n\nmfg_data_2 = mfg_data.groupby(['Machine_ID',mfg_data[\"Timestamp\"].dt.date]).mean(numeric_only=True).reset_index()\nmfg_data_2['Machine_ID'] = mfg_data_2['Machine_ID'].astype('object')\nmfg_data_2['Timestamp'] = pd.to_datetime(mfg_data_2['Timestamp'])\n\nmfg_data_2.head()\n\n\n\n\n\n\n\n\nMachine_ID\nTimestamp\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_%\nQuality_Control_Defect_Rate_%\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_%\n\n\n\n\n0\n1\n2024-01-01\n61.611040\n2.610587\n5.869368\n27.272905\n2.361617\n5.045644\n334.572071\n0.401736\n7.254418\n\n\n1\n1\n2024-01-02\n55.194740\n2.411919\n5.883822\n29.543064\n2.573108\n5.940892\n321.765835\n0.463723\n6.200328\n\n\n2\n1\n2024-01-03\n56.949989\n2.411682\n5.724331\n29.983094\n2.700738\n5.289382\n299.529689\n0.413455\n7.714974\n\n\n3\n1\n2024-01-04\n59.178205\n3.062527\n6.043511\n28.034322\n1.985974\n4.925408\n258.376800\n0.600241\n6.451960\n\n\n4\n1\n2024-01-05\n68.445868\n2.241314\n5.587359\n25.700449\n2.388666\n5.579490\n288.978882\n0.494236\n6.249593\n\n\n\n\n\n\n\n\nmfg_data_2.select_dtypes(include=\"number\").columns.to_list()\nplt.figure(figsize=(10,6))\nsns.pairplot(mfg_data_2, diag_kind='kde',hue='Machine_ID')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nmachine_1 =mfg_data_2[mfg_data_2['Machine_ID']==1] \n\nstart_date = \"2024-01-01\"\nend_date = \"2024-12-30\"\ntred_plot_df = machine_1[(machine_1[\"Timestamp\"] &gt;= start_date) & (machine_1[\"Timestamp\"] &lt;= end_date)]\n\nplt.figure(figsize=(14, 5))\nplt.plot(tred_plot_df[\"Timestamp\"], tred_plot_df[\"Temperature_C\"], marker=\"o\", markersize=3, linestyle=\"-\")\n\nplt.title(\"Temperature (°C) Over Time\", fontsize=14)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Temperature (°C)\")\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n\n\n\n\n\n\n\n\nfor col in mfg_data_2.select_dtypes(include=\"number\").columns.to_list():\n    plot_hist_qq(mfg_data_2,col) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Outliers Overview\ndef report_quantity_outliers(df):\n    for col in df.columns:\n        q25 = df[col].quantile(0.25)\n        q75 = df[col].quantile(0.75)\n        iqr = q75 - q25\n        lower_bound = q25 - 1.5 * iqr\n        upper_bound = q75 + 1.5 * iqr\n        outliers = df[(df[col] &lt; lower_bound) | (df[col] &gt; upper_bound)]\n        print(f\"{col}: {outliers.shape[0]} outliers\")\n\nnum_cols = mfg_data_2.select_dtypes(include=\"number\").columns.to_list()        \n\nreport_quantity_outliers(mfg_data_2[num_cols])\n\nTemperature_C: 30 outliers\nVibration_Hz: 27 outliers\nPower_Consumption_kW: 43 outliers\nNetwork_Latency_ms: 26 outliers\nPacket_Loss_%: 28 outliers\nQuality_Control_Defect_Rate_%: 39 outliers\nProduction_Speed_units_per_hr: 31 outliers\nPredictive_Maintenance_Score: 33 outliers\nError_Rate_%: 29 outliers\n\n\n\nmfg_data_2.to_csv('..\\data\\processed\\mfg_time_series.csv',index=False)"
  },
  {
    "objectID": "presentation.html#quarto",
    "href": "presentation.html#quarto",
    "title": "Project title",
    "section": "Quarto",
    "text": "Quarto\n\nThe presentation is created using the Quarto CLI\n## sets the start of a new slide"
  },
  {
    "objectID": "presentation.html#layouts",
    "href": "presentation.html#layouts",
    "title": "Project title",
    "section": "Layouts",
    "text": "Layouts\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\nlike\nthis\n\nAnd add footnotes"
  },
  {
    "objectID": "presentation.html#code",
    "href": "presentation.html#code",
    "title": "Project title",
    "section": "Code",
    "text": "Code\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Wed, 30 Jul 2025   Prob (F-statistic):           5.84e-08\nTime:                        17:34:35   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "presentation.html#plots",
    "href": "presentation.html#plots",
    "title": "Project title",
    "section": "Plots",
    "text": "Plots"
  },
  {
    "objectID": "presentation.html#plot-and-text",
    "href": "presentation.html#plot-and-text",
    "title": "Project title",
    "section": "Plot and text",
    "text": "Plot and text\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#tables",
    "href": "presentation.html#tables",
    "title": "Project title",
    "section": "Tables",
    "text": "Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\n\n\n\nisland\n\n\n\nbill_length_mm\n\n\n\nbill_depth_mm\n\n\n\nflipper_length_mm\n\n\n\nbody_mass_g\n\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.1\n\n\n\n18.7\n\n\n\n181.0\n\n\n\n3750.0\n\n\n\nMale\n\n\n\n\n\n\n\n1\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.5\n\n\n\n17.4\n\n\n\n186.0\n\n\n\n3800.0\n\n\n\nFemale\n\n\n\n\n\n\n\n2\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n40.3\n\n\n\n18.0\n\n\n\n195.0\n\n\n\n3250.0\n\n\n\nFemale\n\n\n\n\n\n\n\n4\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n36.7\n\n\n\n19.3\n\n\n\n193.0\n\n\n\n3450.0\n\n\n\nFemale\n\n\n\n\n\n\n\n5\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.3\n\n\n\n20.6\n\n\n\n190.0\n\n\n\n3650.0\n\n\n\nMale"
  },
  {
    "objectID": "presentation.html#images",
    "href": "presentation.html#images",
    "title": "Project title",
    "section": "Images",
    "text": "Images\n\nImage credit: Danielle Navarro, Percolate."
  },
  {
    "objectID": "presentation.html#math-expressions",
    "href": "presentation.html#math-expressions",
    "title": "Project title",
    "section": "Math Expressions",
    "text": "Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $\\alpha+\\beta$ renders \\(\\alpha + \\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $."
  },
  {
    "objectID": "presentation.html#feeling-adventurous",
    "href": "presentation.html#feeling-adventurous",
    "title": "Project title",
    "section": "Feeling adventurous?",
    "text": "Feeling adventurous?\n\nYou are welcomed to use the default styling of the slides. In fact, that’s what I expect majority of you will do. You will differentiate yourself with the content of your presentation.\nBut some of you might want to play around with slide styling. Some solutions for this can be found at https://quarto.org/docs/presentations/revealjs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by Cesar Castro M. For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nCesar Castro M. : Second Semester - Master in Data Science - University of Arizona."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "",
    "text": "Add project abstract here."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "",
    "text": "Add project abstract here."
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "",
    "text": "Uncovering Patterns and Anomalies in Manufacturing Data"
  },
  {
    "objectID": "proposal.html#project-description",
    "href": "proposal.html#project-description",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "",
    "text": "Uncovering Patterns and Anomalies in Manufacturing Data"
  },
  {
    "objectID": "proposal.html#goals",
    "href": "proposal.html#goals",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "🎯Goals:",
    "text": "🎯Goals:\nThe construction of modern factories is resulting in the generation of vast amounts of data. Manufacturing equipment continuously monitors various parameters, such as temperatures, vibrations, motor speeds, and energy consumption, using sensors and other methods. Variations in these parameters can indicate shifts in performance, potentially leading to defects or catastrophic failures in the equipment. Detecting these shifts has become increasingly important to reduce downtime and boost productivity.\nAdvanced techniques such as machine learning, anomaly detection, and image analysis are currently being utilized to forecast when equipment might require maintenance, calibration, or material changes. This project aims to leverage synthetic public data from Kaggle to compare various classification and regression models, with the objective of predicting these critical events. If time allows, we will also explore anomaly detection techniques on time series data to predict potential failures as early as possible.\nSpecific Objectives:\n\nFirst objective is to build a classification model for failures (will compare multiple options). The model will analyze sensor data, such as air temperature, process temperature, rotational speed, and torque, from a predictive maintenance dataset to accurately predict the specific fails and failure type.\nSecond objective is to develop a regression model for anomaly detection and compare to time series analysis (e.g. ARIMA, LSTM). This model will use key features like sensor data and performance metrics to identify unusual patterns."
  },
  {
    "objectID": "proposal.html#proposed-datasets",
    "href": "proposal.html#proposed-datasets",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "📊Proposed Datasets:",
    "text": "📊Proposed Datasets:\n\nSource: Kaggle - Machine Predictive Maintenance Classification (Synthetic dataset that reflects real predictive maintenance encountered in the industry)\n\nData Example:\n\n\n\n\n\n\n\n\n\nUDI\nProduct ID\nType\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nTarget\nFailure Type\n\n\n\n\n0\n1\nM14860\nM\n298.1\n308.6\n1551\n42.8\n0\n0\nNo Failure\n\n\n1\n2\nL47181\nL\n298.2\n308.7\n1408\n46.3\n3\n0\nNo Failure\n\n\n2\n3\nL47182\nL\n298.1\n308.5\n1498\n49.4\n5\n0\nNo Failure\n\n\n3\n4\nL47183\nL\n298.2\n308.6\n1433\n39.5\n7\n0\nNo Failure\n\n\n4\n5\nL47184\nL\n298.2\n308.7\n1408\n40.0\n9\n0\nNo Failure\n\n\n\n\n\n\n\n\nExample of data, 3 categorical features and 6 numerical features to be used. This dataset will be used for classification models.\n\n\n\n\n\n\n\n\n\n\nUDI\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nTarget\n\n\n\n\ncount\n10000.00000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n5000.50000\n300.004930\n310.005560\n1538.776100\n39.986910\n107.951000\n0.033900\n\n\nstd\n2886.89568\n2.000259\n1.483734\n179.284096\n9.968934\n63.654147\n0.180981\n\n\nmin\n1.00000\n295.300000\n305.700000\n1168.000000\n3.800000\n0.000000\n0.000000\n\n\n25%\n2500.75000\n298.300000\n308.800000\n1423.000000\n33.200000\n53.000000\n0.000000\n\n\n50%\n5000.50000\n300.100000\n310.100000\n1503.000000\n40.100000\n108.000000\n0.000000\n\n\n75%\n7500.25000\n301.500000\n311.100000\n1612.000000\n46.800000\n162.000000\n0.000000\n\n\nmax\n10000.00000\n304.500000\n313.800000\n2886.000000\n76.600000\n253.000000\n1.000000\n\n\n\n\n\n\n\n\nThere are 10000 rows on the predictive maintenance dataset, max values for Rotational speed and Tool wear might indacate outliers or some level of skewness on the data. This will be handle during the data preparation part of the project.\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 10 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   UDI                      10000 non-null  int64  \n 1   Product ID               10000 non-null  object \n 2   Type                     10000 non-null  object \n 3   Air temperature [K]      10000 non-null  float64\n 4   Process temperature [K]  10000 non-null  float64\n 5   Rotational speed [rpm]   10000 non-null  int64  \n 6   Torque [Nm]              10000 non-null  float64\n 7   Tool wear [min]          10000 non-null  int64  \n 8   Target                   10000 non-null  int64  \n 9   Failure Type             10000 non-null  object \ndtypes: float64(3), int64(4), object(3)\nmemory usage: 781.4+ KB\n\n\n\nThere are no missing values on this dataset\n\n\nSource: Kaggle - Intelligent Manufacturing Dataset (The Intelligent Manufacturing Dataset for Predictive Optimization is a dataset designed for research in smart manufacturing, AI-driven process optimization, and predictive maintenance)\n\nData Example:\n\n\n\n\n\n\n\n\n\nTimestamp\nMachine_ID\nOperation_Mode\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_%\nQuality_Control_Defect_Rate_%\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_%\nEfficiency_Status\n\n\n\n\n0\n2024-01-01 00:00:00\n39\nIdle\n74.137590\n3.500595\n8.612162\n10.650542\n0.207764\n7.751261\n477.657391\n0.344650\n14.965470\nLow\n\n\n1\n2024-01-01 00:01:00\n29\nActive\n84.264558\n3.355928\n2.268559\n29.111810\n2.228464\n4.989172\n398.174747\n0.769848\n7.678270\nLow\n\n\n2\n2024-01-01 00:02:00\n15\nActive\n44.280102\n2.079766\n6.144105\n18.357292\n1.639416\n0.456816\n108.074959\n0.987086\n8.198391\nLow\n\n\n3\n2024-01-01 00:03:00\n43\nActive\n40.568502\n0.298238\n4.067825\n29.153629\n1.161021\n4.582974\n329.579410\n0.983390\n2.740847\nMedium\n\n\n4\n2024-01-01 00:04:00\n8\nIdle\n75.063817\n0.345810\n6.225737\n34.029191\n4.796520\n2.287716\n159.113525\n0.573117\n12.100686\nLow\n\n\n\n\n\n\n\n\nExample of dataset, dates, numerical and categorical variables. This dataset will be used to explore regression models, time series analysis and anomaly detection.\n\n\n\n\n\n\n\n\n\n\nMachine_ID\nTemperature_C\nVibration_Hz\nPower_Consumption_kW\nNetwork_Latency_ms\nPacket_Loss_%\nQuality_Control_Defect_Rate_%\nProduction_Speed_units_per_hr\nPredictive_Maintenance_Score\nError_Rate_%\n\n\n\n\ncount\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n100000.000000\n\n\nmean\n25.499330\n60.041458\n2.549959\n5.745929\n25.555620\n2.493418\n5.008806\n275.916324\n0.499385\n7.504100\n\n\nstd\n14.389439\n17.323238\n1.414127\n2.451271\n14.120758\n1.443273\n2.883666\n130.096892\n0.288814\n4.335896\n\n\nmin\n1.000000\n30.000138\n0.100011\n1.500183\n1.000025\n0.000026\n0.000449\n50.000375\n0.000003\n0.000112\n\n\n25%\n13.000000\n45.031596\n1.323214\n3.627318\n13.355118\n1.245026\n2.521591\n162.873618\n0.248166\n3.750148\n\n\n50%\n25.000000\n60.033597\n2.549441\n5.755460\n25.536079\n2.487667\n5.003569\n276.648922\n0.499209\n7.504145\n\n\n75%\n38.000000\n74.967217\n3.776459\n7.860267\n37.796372\n3.741252\n7.506127\n388.812761\n0.748810\n11.273189\n\n\nmax\n50.000000\n89.998979\n4.999974\n9.999889\n49.999917\n4.999975\n9.999900\n499.996768\n0.999978\n14.999869\n\n\n\n\n\n\n\n\nThere are 100000 rows on this dataset.\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 13 columns):\n #   Column                         Non-Null Count   Dtype  \n---  ------                         --------------   -----  \n 0   Timestamp                      100000 non-null  object \n 1   Machine_ID                     100000 non-null  int64  \n 2   Operation_Mode                 100000 non-null  object \n 3   Temperature_C                  100000 non-null  float64\n 4   Vibration_Hz                   100000 non-null  float64\n 5   Power_Consumption_kW           100000 non-null  float64\n 6   Network_Latency_ms             100000 non-null  float64\n 7   Packet_Loss_%                  100000 non-null  float64\n 8   Quality_Control_Defect_Rate_%  100000 non-null  float64\n 9   Production_Speed_units_per_hr  100000 non-null  float64\n 10  Predictive_Maintenance_Score   100000 non-null  float64\n 11  Error_Rate_%                   100000 non-null  float64\n 12  Efficiency_Status              100000 non-null  object \ndtypes: float64(9), int64(1), object(3)\nmemory usage: 9.9+ MB\n\n\n\nThere are no missing data on any of the features."
  },
  {
    "objectID": "proposal.html#project-schedule",
    "href": "proposal.html#project-schedule",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "🗓️Project Schedule",
    "text": "🗓️Project Schedule\n\nDefinition of problem statement and goals. Due Date: 8/6/2025\nPlan to incorporate peer review feedback into project plan. Due Date: 8/7/2025\nData cleaning. (handling missing, outliers, define imputation methods). Due Date: 8/13/2025\nDefine key response on the datasets, depending on the model (might want to look like defects pass/fail) for a classification model or defect rate for a regression model. Due Date: 8/13/2025\nAnalyze features (use PCA or others to understand which features contribute more the variability, etc.) Due Date: 8/13/2025\nClassification Model Creation and Validation. Due Date: 8/15/2025\nRegression Model Creation and Validation,. Due Date: 8/18/2025\nIncorporate time series analysis and compare models and recommend the best one. . Due Date: 8/18/2025\nPrepare final report and presentation Due Date: 8/20/2025"
  },
  {
    "objectID": "proposal.html#project-organization",
    "href": "proposal.html#project-organization",
    "title": "Uncovering Patterns and Anomalies in Manufacturing Data",
    "section": "📁Project Organization",
    "text": "📁Project Organization\n| FINAL-PROJECT-CASTRO\n| — 📁DATA: # Raw Data files obtained from Kaggle source in CSV format.\n| ______|—- 📁processed: # Cleaned and processed datasets\n| ______|—- 📁results: # model evaluation and other results\n| — 📁IMAGES: # Any images to be used by quarto site\n| — 📁presentation_files: # Quarto presentation files\n| — 📁extra: # Additional documents or files used on project\n| — 📁quarto: # quarto files\n| — 📁src: # source code used for project\n| — 📁.github: # github configuration files\n| – 📄requirements.txt: # Python Dependencies\n| – 📄_quarto.yml # quarto metadata and configuration\n| – 📄.gitignore # list of files and directories to be ignore by Git\n| – 📄about.qmd # Quarto about page with general information about the project\n| – 📄presentation.qmd # Quarto final project presentation\n| – 📄proposal.qmd # Project Problem statement and proposal\n| – 📄README.md # main read me file for git."
  },
  {
    "objectID": "src/Data_Exploration_PDM.html",
    "href": "src/Data_Exploration_PDM.html",
    "title": "Data Exploration/Cleaning for Predictive Maintenance",
    "section": "",
    "text": "Author: Cesar Castro M\n\n\nDate: 08/11/2025\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\n\n\nmfg_data = pd.read_csv('..\\data\\predictive_maintenance.csv')\n\nSource: https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification\n - UID: unique identifier ranging from 1 to 10000  - productID: consisting of a letter L, M, or H for low (50% of all products), medium (30%), and high (20%) as product quality variants and a variant-specific serial number - air temperature [K]: generated using a random walk process later normalized to a standard deviation of 2 K around 300 K - process temperature [K]: generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K. - rotational speed [rpm]: calculated from powepower of 2860 W, overlaid with a normally distributed noise - torque [Nm]: torque values are normally distributed around 40 Nm with an Ïƒ = 10 Nm and no negative values. - tool wear [min]: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process. and a ‘machine failure’ label that indicates, whether the machine has failed in this particular data point for any of the following failure modes are true.\n\nmfg_data.head()\n\n\n\n\n\n\n\n\nUDI\nProduct ID\nType\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nTarget\nFailure Type\n\n\n\n\n0\n1\nM14860\nM\n298.1\n308.6\n1551\n42.8\n0\n0\nNo Failure\n\n\n1\n2\nL47181\nL\n298.2\n308.7\n1408\n46.3\n3\n0\nNo Failure\n\n\n2\n3\nL47182\nL\n298.1\n308.5\n1498\n49.4\n5\n0\nNo Failure\n\n\n3\n4\nL47183\nL\n298.2\n308.6\n1433\n39.5\n7\n0\nNo Failure\n\n\n4\n5\nL47184\nL\n298.2\n308.7\n1408\n40.0\n9\n0\nNo Failure\n\n\n\n\n\n\n\n\n# Setting right feature type\nmfg_data['UDI'] = mfg_data['UDI'].astype('object')\n\n\n#Find Categorical Features on childcare_costs dataset\ndef cat_features_explore(df):\n    def calc_unique_counts(df,col_name):\n        col = df[col_name]\n        unique_val = col.unique()\n        plt.figure(figsize=(12,4))\n        sns.countplot(data=df, x=col_name)\n        plt.title(f'Categories for {col_name}')\n        plt.tight_layout()\n        plt.show()\n        return len(unique_val)\n\n    cat_cols = mfg_data.select_dtypes(include=\"object\").columns.to_list()\n    count_dic = {col:calc_unique_counts(mfg_data,col) for col in cat_cols}\n    print(count_dic)    \n \ncat_features_explore(mfg_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'UDI': 10000, 'Product ID': 10000, 'Type': 3, 'Failure Type': 6}\n\n\n\ndef plot_hist_qq(df,col_name):\n    col = df[col_name]\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2,nrows=1,figsize=(8, 4))\n    sns.histplot(col, linewidth=1,ax=ax1)\n    #sns.kdeplot(col, linewidth=5,ax=ax1)\n    ax1.set_title(col_name + ' Histogram')\n\n    sm.qqplot(col,line='s',ax=ax2)\n    ax2.set_title(col_name + ' Q-Q Plot')\n    plt.tight_layout()\n    plt.show()\n\ndef calculate_stats(df,col_name):\n    print (f\"{col_name} Mean: {df[col_name].mean():.2f}\")\n    print (f\"{col_name} Median: {df[col_name].median():.2f}\")\n    print (f\"{col_name} Max: {df[col_name].max():.2f}\")\n    print (f\"{col_name} Min: {df[col_name].min():.2f}\")\n    print (f\"{col_name} Standard Deviation: {df[col_name].std():.2f}\")    \n    print (f\"{col_name} Number of Points: {len(df[col_name]):.0f}\")    \n\nfor col in mfg_data.select_dtypes(include=\"number\").columns.to_list():\n    plot_hist_qq(mfg_data,col) \n    calculate_stats(mfg_data,col)\n\n\n\n\n\n\n\n\nAir temperature [K] Mean: 300.00\nAir temperature [K] Median: 300.10\nAir temperature [K] Max: 304.50\nAir temperature [K] Min: 295.30\nAir temperature [K] Standard Deviation: 2.00\nAir temperature [K] Number of Points: 10000\n\n\n\n\n\n\n\n\n\nProcess temperature [K] Mean: 310.01\nProcess temperature [K] Median: 310.10\nProcess temperature [K] Max: 313.80\nProcess temperature [K] Min: 305.70\nProcess temperature [K] Standard Deviation: 1.48\nProcess temperature [K] Number of Points: 10000\n\n\n\n\n\n\n\n\n\nRotational speed [rpm] Mean: 1538.78\nRotational speed [rpm] Median: 1503.00\nRotational speed [rpm] Max: 2886.00\nRotational speed [rpm] Min: 1168.00\nRotational speed [rpm] Standard Deviation: 179.28\nRotational speed [rpm] Number of Points: 10000\n\n\n\n\n\n\n\n\n\nTorque [Nm] Mean: 39.99\nTorque [Nm] Median: 40.10\nTorque [Nm] Max: 76.60\nTorque [Nm] Min: 3.80\nTorque [Nm] Standard Deviation: 9.97\nTorque [Nm] Number of Points: 10000\n\n\n\n\n\n\n\n\n\nTool wear [min] Mean: 107.95\nTool wear [min] Median: 108.00\nTool wear [min] Max: 253.00\nTool wear [min] Min: 0.00\nTool wear [min] Standard Deviation: 63.65\nTool wear [min] Number of Points: 10000\n\n\n\n\n\n\n\n\n\nTarget Mean: 0.03\nTarget Median: 0.00\nTarget Max: 1.00\nTarget Min: 0.00\nTarget Standard Deviation: 0.18\nTarget Number of Points: 10000\n\n\n\nnum_cols = mfg_data.select_dtypes(include=\"number\").columns.to_list()\nplot_df = mfg_data[num_cols]\nplt.figure(figsize=(10,6))\nsns.pairplot(plot_df, diag_kind='kde',hue='Target')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n#Standardization\nfrom sklearn.preprocessing import StandardScaler\n\nmfg_data_scaled = mfg_data.copy().drop(columns=['Target'])\n\nscaler = StandardScaler()\nnum_cols = mfg_data_scaled.select_dtypes(include=\"number\").columns.to_list()\n\nmfg_data_scaled[num_cols] = scaler.fit_transform(mfg_data_scaled[num_cols])\nmfg_data_scaled['Target'] = mfg_data['Target']\n\n\nnum_cols = mfg_data_scaled.select_dtypes(include=\"number\").columns.to_list()\nplot_df = mfg_data_scaled[num_cols]\nplt.figure(figsize=(10,6))\nsns.pairplot(plot_df, diag_kind='kde',hue='Target')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\ndfsgsdf=Msdfsdfgdsfhgdsadfgasdfgsdfgsdfgsdfga_3 = mfg_data_scaled.copy()\nmfg_data_3 = mfg_data_3.drop(columns=['UDI']) #High Cardinality\nmfg_data_3 = mfg_data_3.drop(columns=['Product ID']) #High Cardinality\n\nmfg_data_3.head()\n\n\n\n\n\n\n\n\nType\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nFailure Type\nTarget\n\n\n\n\n0\nM\n-0.952389\n-0.947360\n0.068185\n0.282200\n-1.695984\nNo Failure\n0\n\n\n1\nL\n-0.902393\n-0.879959\n-0.729472\n0.633308\n-1.648852\nNo Failure\n0\n\n\n2\nL\n-0.952389\n-1.014761\n-0.227450\n0.944290\n-1.617430\nNo Failure\n0\n\n\n3\nL\n-0.902393\n-0.947360\n-0.590021\n-0.048845\n-1.586009\nNo Failure\n0\n\n\n4\nL\n-0.902393\n-0.879959\n-0.729472\n0.001313\n-1.554588\nNo Failure\n0\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nl_encoder = LabelEncoder()\nmfg_data_3['FailureType_encoded'] = l_encoder.fit_transform(mfg_data_3['Failure Type'])\nmfg_data_3.head()\n\n\n\n\n\n\n\n\nType\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nFailure Type\nTarget\nFailureType_encoded\n\n\n\n\n0\nM\n-0.952389\n-0.947360\n0.068185\n0.282200\n-1.695984\nNo Failure\n0\n1\n\n\n1\nL\n-0.902393\n-0.879959\n-0.729472\n0.633308\n-1.648852\nNo Failure\n0\n1\n\n\n2\nL\n-0.952389\n-1.014761\n-0.227450\n0.944290\n-1.617430\nNo Failure\n0\n1\n\n\n3\nL\n-0.902393\n-0.947360\n-0.590021\n-0.048845\n-1.586009\nNo Failure\n0\n1\n\n\n4\nL\n-0.902393\n-0.879959\n-0.729472\n0.001313\n-1.554588\nNo Failure\n0\n1\n\n\n\n\n\n\n\n\nenconder_mapping = dict(zip(l_encoder.classes_, range(len(l_encoder.classes_))))\n\nprint(enconder_mapping)\n\n{'Heat Dissipation Failure': 0, 'No Failure': 1, 'Overstrain Failure': 2, 'Power Failure': 3, 'Random Failures': 4, 'Tool Wear Failure': 5}\n\n\n\nmfg_data_3 = mfg_data_3.drop(columns=['Failure Type'])\nmfg_data_3 = pd.get_dummies(mfg_data_3)\nmfg_data_3.head()\n\n\n\n\n\n\n\n\nAir temperature [K]\nProcess temperature [K]\nRotational speed [rpm]\nTorque [Nm]\nTool wear [min]\nTarget\nFailureType_encoded\nType_H\nType_L\nType_M\n\n\n\n\n0\n-0.952389\n-0.947360\n0.068185\n0.282200\n-1.695984\n0\n1\nFalse\nFalse\nTrue\n\n\n1\n-0.902393\n-0.879959\n-0.729472\n0.633308\n-1.648852\n0\n1\nFalse\nTrue\nFalse\n\n\n2\n-0.952389\n-1.014761\n-0.227450\n0.944290\n-1.617430\n0\n1\nFalse\nTrue\nFalse\n\n\n3\n-0.902393\n-0.947360\n-0.590021\n-0.048845\n-1.586009\n0\n1\nFalse\nTrue\nFalse\n\n\n4\n-0.902393\n-0.879959\n-0.729472\n0.001313\n-1.554588\n0\n1\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\nmfg_data_3.to_csv('..\\data\\processed\\predictive_maintenance_ready.csv',index=False)"
  }
]