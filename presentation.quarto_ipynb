{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Uncovering Patterns and Anomalies in Manufacturing Data\"\n",
        "subtitle: \"INFO 523 - Summer 2025 - Final Project\"\n",
        "author: \"Cesar Castro M.\"\n",
        "title-slide-attributes:\n",
        "  data-background-image: images/background.png\n",
        "  data-background-size: stretch\n",
        "  data-background-opacity: \"0.7\"\n",
        "  data-slide-number: none\n",
        "format:\n",
        "  revealjs:\n",
        "    theme:  ['data/customtheming.scss']\n",
        "  \n",
        "editor: visual\n",
        "jupyter: python3\n",
        "execute:\n",
        "  echo: false\n",
        "---"
      ],
      "id": "90d420fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-packages\n",
        "#| include: false\n",
        "\n",
        "# Load packages here\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "id": "load-packages",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup\n",
        "#| include: false\n",
        "# Set up plot theme and figure resolution\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "sns.set_context(\"notebook\", font_scale=0.8)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['figure.figsize'] = (6, 6 * 0.618)"
      ],
      "id": "setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load-data\n",
        "#| include: false\n",
        "# Load data in Python\n",
        "mtcars = sns.load_dataset('mpg').dropna()  # mtcars dataset is similar to the mpg dataset from seaborn\n",
        "mtcars['speed'] = mtcars['horsepower'] / mtcars['weight']\n",
        "\n",
        "penguins = sns.load_dataset('penguins').dropna()"
      ],
      "id": "load-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uncovering Patterns and Anomalies in Manufacturing Data\n",
        "\n",
        "## Introduction\n",
        "\n",
        "-   All code used to generate the charts and table on this presentation can be found here:\n",
        "-   https://github.com/INFO-523-SU25/final-project-castro/tree/main/src\n",
        "\n",
        "\n",
        "## Dataset for Machine Learning Classification\n",
        "\n",
        "-   Source: [Machine Predictive Maintenance Classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification)\n",
        "\n",
        "-   Key Features:\n",
        "\n",
        "    ::: {style=\"font-size: 0.5em;\"}\n",
        "    -   UID: unique identifier ranging from 1 to 10000\n",
        "    -   productID: consisting of a letter L, M, or H for low (50% of all products), medium (30%), and high (20%) as product quality variants and a variant-specific serial number\n",
        "    -   Type: is a columns that consist only on the letters L, M and H from productID.\n",
        "    -   air temperature \\[K\\]: generated using a random walk process later normalized to a standard deviation of 2 K around 300 K\n",
        "    -   process temperature \\[K\\]: generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.\n",
        "    -   rotational speed \\[rpm\\]: calculated from powepower of 2860 W, overlaid with a normally distributed noise\n",
        "    -   torque \\[Nm\\]: torque values are normally distributed around 40 Nm with an Ïƒ = 10 Nm and no negative values.\n",
        "    -   tool wear \\[min\\]: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process.\n",
        "    -   Machine failure: label that indicates, whether the machine has failed in this particular data point for any of the following failure modes are true\n",
        "    :::\n",
        "\n",
        "## Manufacturing Data Exploration\n",
        "\n",
        "Example of 5 rows of the synthetic data used for predictive modeling.\n",
        "\n",
        "::: {style=\"font-size: 0.5em;\"}"
      ],
      "id": "2a1c967a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_csv(\"data/predictive_maintenance.csv\")\n",
        "df1.head(5).to_html()"
      ],
      "id": "1a57dc44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Manufacturing Data Exploration\n",
        "\n",
        "Numerical Features Relationship\n",
        "\n",
        ":::::: columns\n",
        "::: {.column width=\"60%\"}"
      ],
      "id": "da873498"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 10,
        "fig-height": 10
      },
      "source": [
        "import pandas as pd\n",
        "mfg_data = pd.read_csv(\"data/predictive_maintenance.csv\")\n",
        "\n",
        "num_cols = mfg_data.select_dtypes(include=\"number\").columns.to_list()\n",
        "plot_df = mfg_data[num_cols]\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.pairplot(plot_df, diag_kind='kde',hue='Target')\n",
        "plt.title('Relationship between numerical features')\n",
        "plt.show()"
      ],
      "id": "6a49ae3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        ":::: {.column width=\"40%\"}\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "-   Failures marked on the pair plot showing stronger relationship with some features.\n",
        "-   There is some non-linear correlation observed between Torque and Rotation speed (suggesting data is not entirely random and mimic a real use case).\n",
        "-   Failures seems on the \"edges\" of the distributions for multiple features which might help models to identify them.\n",
        ":::\n",
        "::::\n",
        "::::::\n",
        "\n",
        "## Manufacturing Data Preparation\n",
        "\n",
        "Data Scaling\n",
        "\n",
        "``` python\n",
        "#Standardization\n",
        "from sklearn.preprocessing import StandardScaler # use StandardScalar from sklearn\n",
        "\n",
        "mfg_data_scaled = mfg_data.copy().drop(columns=['Target'])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "num_cols = mfg_data_scaled.select_dtypes(include=\"number\").columns.to_list()\n",
        "mfg_data_scaled[num_cols] = scaler.fit_transform(mfg_data_scaled[num_cols])\n",
        "```\n",
        "\n",
        "Categorical Features Encoding\n",
        "\n",
        "``` python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "l_encoder = LabelEncoder()\n",
        "mfg_data_3['FailureType_encoded'] = l_encoder.fit_transform(mfg_data_3['Failure Type'])\n",
        "```\n",
        "\n",
        "## Model Training\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Using a method from Chanin Nantasenamat, multiple classification models were compared for ROC-AUC and the F1 score.\n",
        ":::\n",
        "\n",
        "![](images/paste-1.png){width=\"667\"}\n",
        "\n",
        "## Model Selection\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Two models will be evaluated Random Forest Classifer based on the results of the comparison and XGboost based on research model might be proper for imbalance datasets (<https://xgboosting.com/xgboost-for-imbalanced-classification/>)\n",
        ":::\n",
        "\n",
        "![](images/paste-2.png){width=\"953\"}\n",
        "\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Source: Data_Exploration_PDM-ipynb\n",
        ":::\n",
        "\n",
        "## Random Forest Model Tuning\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "For hyper-parameter tuning a combination of *sklearn.model_selection - GridSearchCV and RandomizedSearchCV* was used to run over multiple options.\n",
        ":::\n",
        "``` python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 500),\n",
        "    'max_depth': randint(1, 20),\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 20),\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rand_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=20, cv=5,scoring='f1')\n",
        "rand_search.fit(X_train, y_train)\n",
        "print('Best hyperparameters:', rand_search.best_params_)\n",
        "```\n",
        "\n",
        "## Random Forest Results\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "After tuning the model, the best F1 score obtained is 0.62. Cross-validation results suggest the model is over-fitting and might not be able to generalize.\n",
        "\n",
        "-   RandomForestClassifier: ROC AUC on test dataset: 0.9751\n",
        "\n",
        "-   RandomForestClassifier: f1 score on test dataset: 0.6258\n",
        "\n",
        "Cross Validation Results\n",
        "\n",
        "-   Test_f1 Cross Validation results 0.45\n",
        "\n",
        "-   Train_f1 Cross Validation results 0.83\n",
        ":::\n",
        "\n",
        "## Handling imbalanced dataset\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Different techniques were explored to see if the model would improve over-fitting and also the F1-score. Over sampling using Synthetic Minority Oversampling Technique (SMOTE) and under sampling using RandomUnderSampler from imblearn were used with no significant improvement\n",
        ":::\n",
        "\n",
        "## Random Forest Threshold Optimization\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "To improve the F1 score, a change in the probability threshold was explored; instead of using the standard 0.5, an analysis was done to estimate the ideal point to optimized the F1-Score\n",
        ":::\n",
        "\n",
        "![](images/paste-3.png){width=\"702\"}\n",
        "\n",
        "## Results of Optimized Model\n",
        "\n",
        "![](images/paste-4.png){width=\"870\"}\n",
        "\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "-   After improving the threshold based on the model results, a balance can be found between precision and recall.\n",
        ":::\n",
        "\n",
        "## XGBoost Model Definition\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "A second approach is to use XGBoost; this model has the option to handle weights for each class. By adding a higher weight to the minority class, it is expected to handle the imbalance in the dataset better.\n",
        ":::\n",
        "\n",
        "``` python\n",
        "import xgboost as xgb\n",
        "\n",
        "scale_pos_weight = (len(y0) - np.sum(y0)) / np.sum(y0) #Intent is to give more weight to the minority class (1s on this case)\n",
        "\n",
        "xgmodel = xgb.XGBClassifier(\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgmodel.fit(X_train, y_train)\n",
        "y_pred = xgmodel.predict(X_test)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1 Score: {f1:.3f}\")\n",
        "```\n",
        "\n",
        "## XGBoost Model Results\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "The F1 score for XGBoost without significant tuning and using weights is slightly better than the random forest model originally used.\n",
        ":::\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"40%\"}\n",
        "Model Results\n",
        "\n",
        "-   F1 Score: 0.757\n",
        "\n",
        "Cross Validation\n",
        "\n",
        "-   Test_f1 Cross Validation results 0.73\n",
        "\n",
        "-   Train_f1 Cross Validation results 1.00\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "![](images/paste-7.png){width=\"738\"}\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## XGBoost MultiClass Model\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "XGBoost results were slightly better than Random Forest. Multi-class model will be created to predict the failure types:\n",
        ":::\n",
        "\n",
        "``` python\n",
        "# Defining weights for each class.\n",
        "classes, counts = np.unique(y_train, return_counts=True)\n",
        "inv_freq = 1.0 / counts\n",
        "class_weights = dict(zip(classes, inv_freq))\n",
        "sample_weights = np.array([class_weights[label] for label in y_train])\n",
        "```\n",
        "\n",
        "## XGBoost Model Results\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"40%\"}\n",
        "Model Results\n",
        "\n",
        "-   F1 score per class:\n",
        "\n",
        "    -   Heat-Dissipation: 0.95\n",
        "    -   No Failure: 0.99\n",
        "    -   Overstrain Failure 0.76\n",
        "    -   Powe Failure: 0.8\n",
        "    -   Random Failures and Tool Wear Failure are both 0.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "![](images/paste-9.png){width=\"690\"}\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## XGBoost Features of Importance\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Although the model performance is not great for all classes, understanding what features are important in the prediction model can help subject matter experts interpret results and take action to reduce failures and improve the process overall.\n",
        ":::\n",
        "\n",
        "![](images/paste-10.png){width=\"649\"}\n",
        "\n",
        "## Time Series Analysis\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "The second dataset consists of a simulated real-time sensor data from industrial machines. Source is also from Kaggle and it can be found here: <https://www.kaggle.com/datasets/ziya07/intelligent-manufacturing-dataset/data>\n",
        ":::\n",
        "\n",
        "Key Features:\n",
        "\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "```         \n",
        "-   Industrial IoT Sensor Data\n",
        "    -   Temperature_C, Vibration_Hz, Power_Consumption_kW,\n",
        "-   Network Performance:\n",
        "    -   Network_Latency_ms, Packet_Loss_%, Quality_Control_Defect_Rate_%\n",
        "-   Production Indicators:\n",
        "    -   Production_Speed_units_per_hr, Predictive_Maintenance_Score, Error_Rate_%\n",
        "-   Target Column Efficiency_StatusKey Features:\n",
        "```\n",
        ":::\n",
        "\n",
        "## Time Serie Data Exploration\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Example of 5 rows of the synthetic data used for predictive modeling.\n",
        ":::\n",
        "\n",
        "::: {style=\"font-size: 0.5em;\"}"
      ],
      "id": "f737c985"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd \n",
        "df1 = pd.read_csv(\"data/manufacturing_6G_dataset.csv\") \n",
        "df1.head(5).to_html()"
      ],
      "id": "8298e3b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Time Serie Data Exploration cont.\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Example distribution for one Feature\n",
        ":::\n",
        "\n",
        "![](images/paste-11.png){width=\"808\"}\n",
        "\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Data seems randomly generated instead of coming from a real scenario.\n",
        ":::\n",
        "\n",
        "## Time Serie Data Exploration cont.\n",
        "\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "In order to make the data more like a real scenario, a mean was calculated for every 12 hours.\n",
        ":::\n",
        "\n",
        "![](images/paste-12.png){width=\"829\"}\n",
        "\n",
        "## Time Serie Data Exploration cont.\n",
        "\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Example Trend for Power_Consumption_kW for one of the machines.\n",
        ":::\n",
        "\n",
        "![](images/paste-14.png)\n",
        "\n",
        "## Time Serie Data Analysis\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "After pre-processing the data, the study is focused on understanding how algorithms like the seasonal_decompose library and ARIMA (or auto_arima) can be used for anomaly detection.\n",
        ":::\n",
        "\n",
        "## Time Serie Data Seasonal-Decomposition\n",
        "\n",
        "``` python\n",
        "#https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "# Decompose the time series\n",
        "decomposition = seasonal_decompose(ts_df['Power_Consumption_kW'], model = 'additive',period=14) # 7days for period\n",
        "```\n",
        "\n",
        "![](images/paste-15.png)\n",
        "\n",
        "## Time Serie Data Seasonal-Decomposition\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Anomaly detection using Residuals:\n",
        ":::\n",
        "\n",
        "``` python\n",
        "# Anomalies in residuals\n",
        "residuals = decomposition.resid.dropna() #Obtain residuals from decompositions\n",
        "threshold = 2 * residuals.std() # Our rule, on this cases based on research we selected to use 2X the standard deviation of the residuals\n",
        "anomalies = np.abs(residuals) > threshold # Applying the rule to obtain the anomalies\n",
        "```\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "4 Data points identified as anomalies using this method\n",
        ":::\n",
        "\n",
        "## Time Serie Data ARIMA\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Model Fitting\n",
        ":::\n",
        "\n",
        "``` python\n",
        "#Fitting auto-arima model\n",
        "from pmdarima import auto_arima\n",
        "auto_model = auto_arima(ts_df['Power_Consumption_kW'], \n",
        "                        seasonal=False,\n",
        " \n",
        "                        error_action = 'ignore',  \n",
        "                        suppress_warnings = True, \n",
        "                        stepwise = True)\n",
        "print(auto_model.summary())\n",
        "print(f\"Best (p, d, q): ({auto_model.order[0]}, {auto_model.order[1]}, {auto_model.order[2]})\")\n",
        "```\n",
        "\n",
        "## Time Serie Data ARIMA - Results\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Results from Auto-Arima Model Fitting\n",
        ":::\n",
        "\n",
        "![](images/paste-16.png){width=\"943\"}\n",
        "\n",
        "## Time Serie Data ARIMA - Results Cont.\n",
        "\n",
        "::: {style=\"font-size: 0.7em;\"}\n",
        "Results from the predictions are almost a constant value around the center of the distribution, meaning the ARIMA model is closely predicting the mean for every single value of the time series data. One reason for this could be that the data are not predictable.\n",
        ":::\n",
        "\n",
        "![](images/paste-17.png)\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "::: {style=\"font-size: 0.5em;\"}\n",
        "-   The study demonstrated the application of concepts in machine learning to a common manufacturing problem.\n",
        "\n",
        "-   The Random Forest Classifier model ROC-AUC scores are high, indicating the model can differentiate effectively between fails and no-fails. However, in a real manufacturing process, the majority of the results are positive/pass or no fails, making this indicator not the best for this case. Recall and precision are more appropriate for this case; depending on the use case, we would want to tune the model in one or the other direction or use the F1-score to optimize both.\n",
        "\n",
        "-   The Random Forest model performed poorly for the F1 score when using a standard threshold of 0.5. The study demonstrated this can be improved by selecting an optimized threshold based on the model results.\n",
        "\n",
        "-   Sampling can be a useful method to handle imbalanced datasets; however, in this specific case, it did not provide a significant improvement in model performance.\n",
        "\n",
        "-   Assigning weights to each classes to handle the imbalance sample in combination with gradient boosting (XGboost) model, resulted in better results for F1-Score.\n",
        "\n",
        "-   Multi-class classification results using the learnings from the binary-classification were demonstrated. Due to the nature and frequency of the failures and their relationship with the input features, two classes had zero F1-scores, meaning the model was not able to predict these. Other classes had an F1-score between 0.5 and 0.7, similar to the binary classification results.\n",
        "\n",
        "-   The study demonstrated how seasonal-decomposition and ARIMA can be used for anomaly detection in a real manufacturing use case; results showed how data points deviating the most from the center/target can be identified by using these methods. These methods might not be the best for a process where there are no patterns and data might just have random variability from the target.\n",
        "::: "
      ],
      "id": "f05c1bde"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\cesar\\AppData\\Roaming\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}